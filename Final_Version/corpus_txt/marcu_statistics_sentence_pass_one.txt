From: AAAI-00 Proceedings. Copyright © 2000, AAAI (www.aaai.org). All rights reserved.

Statistics-Based Summarization — Step One: Sentence Compression

                                     Kevin Knight and Daniel Marcu

                            Information Sciences Institute and Department of Computer Science
                                                   University of Southern California
                                                    4676 Admiralty Way, Suite 1001
                                                        Marina del Rey, CA 90292
                                                          {knight,marcu}@isi.edu

                             Abstract                          rules for deleting information that is redundant, com-
                                                               pressing long sentences into shorter ones, aggregating
   When humans produce summaries of documents, they            sentences, repairing reference links, etc.
   do not simply extract sentences and concatenate them.
   Rather, they create new sentences that are grammati-           Our goal is also to generate coherent abstracts. How-
   cal, that cohere with one another, and that capture the     ever, in contrast with the above work, we intend to
   most salient pieces of information in the original doc-     eventually use Abstract, Text tuples, which are widely
   ument. Given that large collections of text/abstract        available, in order to automatically learn how to rewrite
   pairs are available online, it is now possible to envision  Texts as coherent Abstracts. In the spirit of the work
   algorithms that are trained to mimic this process. In       in the statistical MT community, which is focused on
   this paper, we focus on sentence compression, a sim-        sentence-to-sentence translations, we also decided to fo-
   pler version of this larger challenge. We aim to achieve    cus ﬁrst on a simpler problem, that of sentence compres-
   two goals simultaneously: our compressions should be        sion. We chose this problem for two reasons:
   grammatical, and they should retain the most impor-
   tant pieces of information. These two goals can con-        • First, the problem is complex enough to require the
   ﬂict. We devise both noisy-channel and decision-tree           development of sophisticated compression models:
   approaches to the problem, and we evaluate results             Determining what is important in a sentence and
   against manual compressions and a simple baseline.             determining how to convey the important informa-
                                                                  tion grammatically, using only a few words, is just a
                    Introduction                                  scaled down version of the text summarization prob-
                                                                  lem. Yet, the problem is simple enough, since we do
Most of the research in automatic summarization                   not have to worry yet about discourse related issues,
has focused on extraction, i.e., on identifying the               such as coherence, anaphors, etc.
most important clauses/sentences/paragraphs in texts
(see (Mani & Maybury 1999) for a representative col-           • Second, an adequate solution to this problem has
lection of papers). However, determining the most im-             an immediate impact on several applications. For
portant textual segments is only half of what a summa-            example, due to time and space constraints, the
rization system needs to do because, in most cases, the           generation of TV captions often requires only the
simple catenation of textual segments does not yield              most important parts of sentences to be shown on a
coherent outputs. Recently, a number of researchers               screen (Linke-Ellis 1999; Robert-Ribes et al. 1999).
have started to address the problem of generating co-             A good sentence compression module would there-
herent summaries: McKeown et al. (1999), Barzilay et              fore have an impact on the task of automatic cap-
al. (1999), and Jing and McKeown (1999) in the context            tion generation. A sentence compression module
of multidocument summarization; Mani et al. (1999) in             can also be used to provide audio scanning ser-
the context of revising single document extracts; and             vices for the blind (Grefenstette 1998). In gen-
Witbrock and Mittal (1999) in the context of headline             eral, since all systems aimed at producing coher-
generation.                                                       ent abstracts implement manually written sets of
                                                                  sentence compression rules (McKeown et al. 1999;
   The approach proposed by Witbrock and Mit-                     Mani, Gates, & Bloedorn 1999; Barzilay, McKeown,
tal (1999) is the only one that applies a probabilistic           & Elhadad 1999), it is likely that a good sentence
model trained directly on Headline, Document pairs.               compression module would impact the overall quality
However, this model has yet to scale up to generat-               of these systems as well. This becomes particularly
ing multiple-sentence abstracts as well as well-formed,           important for text genres that use long sentences.
grammatical sentences. All other approaches employ
sets of manually written or semi-automatically derived            In this paper, we present two approaches to the sen-
                                                               tence compression problem. Both take as input a se-
Copyright c 2000, American Association for Artiﬁcial In-       quence of words W = w1, w2, . . . , wn (one sentence).
telligence (www.aaai.org). All rights reserved.
An algorithm may drop any subset of these words. The            It is advantageous to break the problem down this
words that remain (order unchanged) form a compres-          way, as it decouples the somewhat independent goals
sion. There are 2n compressions to choose from—some          of creating a short text that (1) looks grammatical,
are reasonable, most are not. Our ﬁrst approach de-          and (2) preserves important information. It is easier to
velops a probabilistic noisy-channel model for sentence      build a channel model that focuses exclusively on the
compression. The second approach develops a decision-        latter, without having to worry about the former. That
based, deterministic model.                                  is, we can specify that a certain substring may represent
                                                             unimportant information, but we do not need to worry
   A noisy-channel model for sentence                        that deleting it will result in an ungrammatical struc-
                                                             ture. We leave that to the source model, which worries
                     compression                             exclusively about well-formedness. In fact, we can make
                                                             use of extensive prior work in source language modeling
This section describes a probabilistic approach to the       for speech recognition, machine translation, and natu-
compression problem. In particular, we adopt the noisy       ral language generation. The same goes for actual com-
channel framework that has been relatively successful in     pression (“decoding” in noisy-channel jargon)—we can
a number of other NLP applications, including speech         re-use generic software packages to solve problems in all
recognition (Jelinek 1997), machine translation (Brown       these application domains.
et al. 1993), part-of-speech tagging (Church 1988),
transliteration (Knight & Graehl 1998), and informa-         Statistical Models
tion retrieval (Berger & Laﬀerty 1999).
                                                             In the experiments we report here, we build very sim-
   In this framework, we look at a long string and imag-     ple source and channel models. In a departure from
ine that (1) it was originally a short string, and then      the above discussion and from previous work on statis-
(2) someone added some additional, optional text to it.      tical channel models, we assign probabilities Ptree(s)
Compression is a matter of identifying the original short    and Pexpand tree(t | s) to trees rather than strings. In
string. It is not critical whether or not the “original”     decoding a new string, we ﬁrst parse it into a large tree t
string is real or hypothetical. For example, in statistical  (using Collins’ parser (1997)), and we then hypothesize
machine translation, we look at a French string and say,     and rank various small trees.
“This was originally English, but someone added ‘noise’
to it.” The French may or may not have been translated          Good source strings are ones that have both (1) a
from English originally, but by removing the noise, we       normal-looking parse tree, and (2) normal-looking word
can hypothesize an English source—and thereby trans-         pairs. Ptree(s) is a combination of a standard proba-
late the string. In the case of compression, the noise       bilistic context-free grammar (PCFG) score, which is
consists of optional text material that pads out the core    computed over the grammar rules that yielded the tree
signal. For the larger case of text summarization, it may    s, and a standard word-bigram score, which is com-
be useful to imagine a scenario in which a news editor       puted over the leaves of the tree. For example, the
composes a short document, hands it to a reporter, and       tree s =(S (NP John) (VP (VB saw) (NP Mary))) is
tells the reporter to “ﬂesh it out” . . . which results in   assigned a score based on these factors:
the article we read in the newspaper. As summarizers,
we may not have access to the editor’s original version          Ptree(s) = P(TOP → S | TOP) ·
(which may or may not exist), but we can guess at it—            P(S → NP VP | S) · P(NP → John | NP) ·
which is where probabilities come in.                            P(VP → VB NP | VP) · P(VP → saw | VB) ·
                                                                 P(NP → Mary | NP) ·
   As in any noisy channel application, we must solve            P(John | EOS) · P(saw | John) ·
three problems:                                                  P(Mary | saw) · P(EOS | Mary)

• Source model. We must assign to every string s a              Our stochastic channel model performs minimal op-
   probability P(s), which gives the chance that s is gen-   erations on a small tree s to create a larger tree t. For
   erated as an “original short string” in the above hy-     each internal node in s, we probabilistically choose an
   pothetical process. For example, we may want P(s)         expansion template based on the labels of the node and
   to be very low if s is ungrammatical.                     its children. For example, when processing the S node
                                                             in the tree above, we may wish to add a prepositional
• Channel model. We assign to every pair of strings          phrase as a third child. We do this with probability
    s, t a probability P(t | s), which gives the chance      P(S → NP VP PP | S → NP VP). Or we may choose
   that when the short string s is expanded, the result      to leave it alone, with probability P(S → NP VP | S →
   is the long string t. For example, if t is the same       NP VP). After we choose an expansion template, then
   as s except for the extra word “not,” then we may         for each new child node introduced (if any), we grow a
   want P(t | s) to be very low. The word “not” is not       new subtree rooted at that node—for example (PP (P
   optional, additional material.                            in) (NP Pittsburgh)). Any particular subtree is grown
                                                             with probability given by its PCFG factorization, as
• Decoder. When we observe a long string t, we search        above (no bigrams).
   for the short string s that maximizes P(s | t). This
   is equivalent to searching for the s that maximizes
   P(s) · P (t | s).
         G                G                 G                 The documentation is typical of Epson quality: excellent.
                                                              Documentation is excellent.
H                A     H        A        F        D
                                                              All of our design goals were achieved and the delivered
aCB                 D  a     C     D  H     Ke                performance matches the speed of the underlying device.
                                                              All design goals were achieved.
         bQ R e              b     e  a     b
                                                              Reach’s E-mail product, MailMan, is a message- manage-
            Zd                                                ment system designed initially for VINES LANs that will
                                                              eventually be operating system-independent.
            c                                                 MailMan will eventually be operating system-independent.

            (t)           (s1)              (s2)              Although the modules themselves may be physically and/or
                                                              electrically incompatible, the cable-speciﬁc jacks on them
   Figure 1: Examples of parse trees.                         provide industry-standard connections.
                                                              Cable-speciﬁc jacks provide industry-standard connections.
Example
                                                              Ingres/Star prices start at $2,100.
In this section, we show how to tell whether one poten-       Ingres/Star prices start at $2,100.
tial compression is more likely than another, according
to the statistical models described above. Suppose we               Figure 2: Examples from our parallel corpus.
observe the tree t in Figure 1, which spans the string
abcde. Consider the compression s1, which is shown in             P(G → H A | G → H A)
the same ﬁgure.                                                    P(A → C B D | A → C B D)

   We compute the factors Ptree(s1) and                            P(B → Q R | B → Q R)
Pexpand tree(t | s1). Breaking this down further,
the source PCFG and word-bigram factors, which                     P(Q → Z | Q → Z)
describe Ptree(s1), are:
                                                                 Now we can simply compare Pexpand tree(s1 |
P(TOP → G | TOP) P(H → a | H)                                 t) = Ptree(s1) · Pexpand tree(t | s1))/Ptree(t) ver-
                                                              sus Pexpand tree(t | t) = Ptree(t) · Pexpand tree(t |
P(G → H A | G)            P(C → b | C)                        t))/Ptree(t) and select the more likely one. Note that
                                                              Ptree(t) and all the PCFG factors can be canceled out,
P(A → C D | A)            P(D → e | D)                        as they appear in any potential compression. Therefore,
                                                              we need only compare compressions of the basis of the
P(a | EOS)                 P(e | b)                           expansion-template probabilities and the word-bigram
P(b | a)                  P(EOS | e)                          probabilities. The quantities that diﬀer between the
                                                              two proposed compressions are boxed above. There-
The channel expansion-template factors and the chan-          fore, s1 will be preferred over t if and only if:
nel PCFG (new tree growth) factors, which describe
Pexpand tree(t | s1), are:                                        P(e | b) · P(A → C B D | A → C D) >
                                                                  P(b | a) · P(c | b) · P(d | c) ·
P(G → H A | G → H A)                  P(Z → c | Z)                P(A → C B D | A → C B D) ·
 P(A → C B D | A → C D)               P(R → d | R)                P(B → Q R | B → Q R) · P(Q → Z | Q → Z)

P(B → Q R | B)                                                Training Corpus
P(Q → Z | Q)                                                  In order to train our system, we used the Ziﬀ-Davis
                                                              corpus, a collection of newspaper articles announcing
   A diﬀerent compression will be scored with a diﬀerent      computer products. Many of the articles in the corpus
set of factors. For example, consider a compression of        are paired with human written abstracts. We automat-
t that leaves t completely untouched. In that case, the       ically extracted from the corpus a set of 1067 sentence
source costs Ptree(t) are:                                    pairs. Each pair consisted of a sentence t = t1, t2, . . . , tn
                                                              that occurred in the article and a possibly compressed
P(TOP → G | TOP)          P(H → a | H)            P(a | EOS)  version of it s = s1, s2, . . . , sm, which occurred in the
P(G → H A | G)            P(C → b | C)             P(b | a)   human written abstract. Figure 2 shows a few sentence
                                                              pairs extracted from the corpus.
P(A → C D | A)            P(Z → c | Z)             P(c | b)
                                                                 We decided to use such a corpus because it is con-
P(B → Q R | B)            P(R → d | R)             P(d | c)   sistent with two desiderata speciﬁc to summarization
P(Q → Z | Q)              P(D → e | D)            P(e | d)    work: (i) the human-written Abstract sentences are
                                                  P(EOS | e)

The channel costs Pexpand tree(t | t) are:
grammatical; (ii) the Abstract sentences represent in a    semantic representation into a vast number of potential
compressed form the salient points of the original news-   English renderings. These renderings are packed into
paper Sentences. We decided to keep in the corpus un-      a forest, from which the most promising sentences are
compressed sentences as well, since we want to learn       extracted using statistical scoring.
not only how to compress a sentence, but also when to
do it.                                                        For our purposes, the extractor selects the trees with
                                                           the best combination of word-bigram and expansion-
Learning Model Parameters                                  template scores. It returns a list of such trees, one for
                                                           each possible compression length. For example, for
We collect expansion-template probabilities from our       the sentence Beyond that basic level, the operations of
parallel corpus. We ﬁrst parse both sides of the parallel  the three products vary, we obtain the following “best”
corpus, and then we identify corresponding syntactic       compressions, with negative log-probabilities shown in
nodes. For example, the parse tree for one sentence        parentheses (smaller = more likely):
may begin (S (NP . . . ) (VP . . . ) (PP . . . )) while
the parse tree for its compressed version may begin (S     Beyond that basic level, the operations of the three products vary
(NP . . . ) (VP . . . )). If these two S nodes are deemed
to correspond, then we chalk up one joint event (S →       widely (1514588)
NP VP, S → NP VP PP); afterwards we normalize.
Not all nodes have corresponding partners; some non-       Beyond that level, the operations of the three products vary widely
correspondences are due to incorrect parses, while oth-
ers are due to legitimate reformulations that are beyond   (1430374)
the scope of our simple channel model. We use standard
methods to estimate word-bigram probabilities.             Beyond that basic level, the operations of the three products vary

Decoding                                                   (1333437)

There is a vast number of potential compressions of a      Beyond that level, the operations of the three products vary
large tree t, but we can pack them all eﬃciently into a
shared-forest structure. For each node of t that has n     (1249223)
children, we
                                                           Beyond that basic level, the operations of the products vary
• generate 2n − 1 new nodes, one for each non-empty
   subset of the children, and                             (1181377)

• pack those nodes so that they are referred to as a       The operations of the three products vary widely (939912)
   whole.
                                                           The operations of the products vary widely (872066)
For example, consider the large tree t above. All com-
pressions can be represented with the following forest:    The operations of the products vary (748761)

G→HA  B→R    A→BC  H→a                                     The operations of products vary (690915)
G→H   Q→Z    A→C   C→b
G→A   A→CBD  A→B   Z→c                                     Operations of products vary (809158)
B→QR  A→CB   A→D   R→d
B→Q   A→CD         D→e                                     The operations vary (522402)

   We can also assign an expansion-template probability    Operations vary (662642)
to each node in the forest. For example, to the B →
Q node, we can assign P(B → Q R | B → Q). If the           Length Selection
observed probability from the parallel corpus is zero,
then we assign a small ﬂoor value of 10−6. In reality,     It is useful to have multiple answers to choose from, as
we produce forests that are much slimmer, as we only       one user may seek a 20% compression, while another
consider compressing a node in ways that are locally       seeks a 60% compression. However, for purposes of
grammatical according to the Penn Treebank—if a rule       evaluation, we want our system to be able to select a
of the type A → C B has never been observed, then it       single compression. If we rely on the log-probabilities
will not appear in the forest.                             as shown above, we will almost always choose the short-
                                                           est compression. (Note above, however, how the three-
   At this point, we want to extract a set of high-        word compression scores better than the two-word com-
scoring trees from the forest, taking into account         pression, as the models are not entirely happy removing
both expansion-template probabilities and word-bigram      the article “the”). To create a more fair competition,
probabilities. Fortunately, we have such a generic ex-     we divide the log-probability by the length of the com-
tractor on hand (Langkilde 2000). This extractor was       pression, rewarding longer strings. This is commonly
designed for a hybrid symbolic-statistical natural lan-    done in speech recognition.
guage generation system called Nitrogen. In that ap-
plication, a rule-based component converts an abstract        If we plot this normalized score against compression
                                                           length, we usually observe a (bumpy) U-shaped curve,
                                                           as illustrated in Figure 3. In a typical more diﬃcult
                                                           case, a 25-word sentence may be optimally compressed
                                                           by a 17-word version. Of course, if a user requires a
                                                           shorter compression than that, she may select another
                                                           region of the curve and look for a local minimum.

                                                              A decision-based model for sentence
                                                                                compression

                                                           In this section, we describe a decision-based, history
                                                           model of sentence compression. As in the noisy-channel
                                                           approach, we again assume that we are given as input
Adjusted negative log-probability of best                                                                              Stack Input List                                  Stack Input List
   compression s at a particular length n
                                                                                                                                                                             F  B      D
       -log P(s) P( t | s) / n                                                                                                                                           HK
                                Advantage is distance .                                                                   G                                              ab     QRe        DROP B

                                      Another advantage is distance .                                                     HA                                                    Zd                    STEP 6
                                             Advantage of broadband is distance .                                                                                               c
                                                    Another advantage of broadband is distance .                          a CB                  D  SHIFT;                                  SHIFT;
                                                                                                                                                   ASSIGNTYPE H                            ASSIGNTYPE D
                                                          Finally another advantage of broadband is distance .               b QRe
                                                                Finally, another advantage of broadband is distance .                                         STEPS 1-2                              STEPS 7-8
                                                                                                                                            Zd                                     FD
                                                                                                                                            c      SHIFT;                      H Ke        REDUCE 2 G
                                                                                                                                                   ASSIGNTYPE K                ab                    STEP 9
                                                                                                                       HA                                                    FD
                                                                                                                                                              STEPS 3-4  HK e
                                                                                                                       a  C  B              D                            ab
                                                                                                                          b                        REDUCE 2 F
                                                                                                                             QRe                              STEP 5             G
                                                                                                                                                                             FD
                                                                                                                             Zd                                          HK e
                                                                                                                                                                         ab
                                                                                                                             c

                 0.20                                                                                                  H KB              D

                 0.15                                                                                                             QRe
                                                                                                                       a bZ d
                  0.10
                                                                                                                          c
                                      4 5 6 78 9
                                               Compression length n                                                    Figure 4: Example of incremental tree compression.

Figure 3: Adjusted log-probabilities for top-scoring                                                                      input list all words that are spanned by constituent
compressions at various lengths (lower is better).                                                                        x in t.

a parse tree t. Our goal is to “rewrite” t into a smaller                                                              • assignType operations are used to change the label
tree s, which corresponds to a compressed version of the                                                                  of trees at the top of the stack. These actions assign
original sentence subsumed by t. Suppose we observe in                                                                    POS tags to the words in the compressed sentence,
our corpus the trees t and s2 in Figure 1. In this model,                                                                 which may be diﬀerent from the POS tags in the
we ask ourselves how we may go about rewriting t into                                                                     original sentence.
s2. One possible solution is to decompose the rewriting
operation into a sequence of shift-reduce-drop actions                                                                 The decision-based model is more ﬂexible than the
that are speciﬁc to an extended shift-reduce parsing                                                                   channel model because it enables the derivation of trees
paradigm.                                                                                                              whose skeleton can diﬀer quite drastically from that of
                                                                                                                       the tree given as input. For example, using the channel
   In the model we propose, the rewriting process starts                                                               model, we are unable to obtain tree s2 from t. However,
with an empty Stack and an Input List that contains the                                                                the four operations listed above enable us to rewrite a
sequence of words subsumed by the large tree t. Each                                                                   tree t into any tree s, as long as an in-order traversal of
word in the input list is labeled with the name of all syn-                                                            the leaves of s produces a sequence of words that occur
tactic constituents in t that start with it (see Figure 4).                                                            in the same order as the words in the tree t. For exam-
At each step, the rewriting module applies an opera-                                                                   ple, the tree s2 can be obtained from tree t by following
tion that is aimed at reconstructing the smaller tree s2.                                                              this sequence of actions, whose eﬀects are shown in Fig-
In the context of our sentence-compression module, we                                                                  ure 4: shift; assignType H; shift; assignType K;
need four types of operations:
• shift operations transfer the ﬁrst word from the in-                                                                 reduce 2 F; drop B; shift; assignType D; reduce

   put list into the stack;                                                                                            2 G.
• reduce operations pop the k syntactic trees located                                                                     To save space, we show shift and assignType op-

   at the top of the stack; combine them into a new                                                                    erations on the same line; however, the reader should
   tree; and push the new tree on the top of the stack.                                                                understand that they correspond to two distinct ac-
   Reduce operations are used to derive the structure of                                                               tions. As one can see, the assignType K operation
   the syntactic tree of the short sentence.                                                                           rewrites the POS tag of the word b; the reduce op-
• drop operations are used to delete from the input list                                                               erations modify the skeleton of the tree given as input.
   subsequences of words that correspond to syntactic                                                                  To increase readability, the input list is shown in a for-
   constituents. A drop x operations deletes from the                                                                  mat that resembles as closely as possible the graphical
                                                                                                                       representation of the trees in ﬁgure 1.

                                                                                                                       Learning the parameters of the
                                                                                                                       decision-based model

                                                                                                                       We associate with each conﬁguration of our shift-
                                                                                                                       reduce-drop, rewriting model a learning case. The cases
                                                                                                                       are generated automatically by a program that derives
                                                                                                                       sequences of actions that map each of the large trees in
                                                                                                                       our corpus into smaller trees. The rewriting procedure
                                                                                                                       simulates a bottom-up reconstruction of the smaller
                                                                                                                       trees.

                                                                                                                          Overall, the 1067 pairs of long and short sentences
                                                                                                                       yielded 46383 learning cases. Each case was labeled
with one action name from a set of 210 possible ac-                              Evaluation
tions: There are 37 distinct assignType actions, one
for each POS tag. There are 63 distinct drop actions,      To evaluate our compression algorithms, we randomly
one for each type of syntactic constituent that can be     selected 32 sentence pairs from our parallel corpus,
deleted during compression. There are 109 distinct re-     which we will refer to as the Test Corpus. We used the
duce actions, one for each type of reduce operation that   other 1035 sentence pairs for training. Figure 5 shows
is applied during the reconstruction of the compressed     three sentences from the Test Corpus, together with the
sentence. And there is one shift operation. Given a        compressions produced by humans, our compression al-
tree t and an arbitrary conﬁguration of the stack and      gorithms, and a baseline algorithm that produces com-
input list, the purpose of the decision-based classiﬁer    pressions with highest word-bigram scores. The exam-
is to learn what action to choose from the set of 210      ples are chosen so as to reﬂect good, average, and bad
possible actions.                                          performance cases. The ﬁrst sentence is compressed in
                                                           the same manner by humans and our algorithms (the
   To each learning example, we associated a set of 99     baseline algorithm chooses though not to compress this
features from the following two classes:                   sentence). For the second example, the output of the
                                                           Decision-based algorithm is grammatical, but the se-
Operational features reﬂect the number of trees            mantics is negatively aﬀected. The noisy-channel al-
   in the stack, the input list, and the types of          gorithm deletes only the word “break”, which aﬀects
   the last ﬁve operations. They also encode infor-        the correctness of the output less. In the last example,
   mation that denote the syntactic category of the        the noisy-channel model is again more conservative and
   root nodes of the partial trees built up to a cer-      decides not to drop any constituents. In constrast, the
   tain time. Examples of such features are: num-          decision-based algorithm compresses the input substan-
   berTreesInStack, wasPreviousOperationShift, syn-        tially, but it fails to produce a grammatical output.
   tacticLabelOfTreeAtTheTopOfStack, etc.
                                                              We presented each original sentence in the Test Cor-
Original-tree-speciﬁc features denote the syntac-          pus to four judges, together with four compressions of it:
   tic constituents that start with the ﬁrst unit in the   the human generated compression, the outputs of the
   input list. Examples of such features are: inputList-   noisy-channel and decision-based algorithms, and the
   StartsWithA CC, inputListStartsWithA PP, etc.           output of the baseline algorithm. The judges were told
                                                           that all outputs were generated automatically. The or-
   The decision-based compression module uses the          der of the outputs was scrambled randomly across test
C4.5 program (Quinlan 1993) in order to learn deci-        cases.
sion trees that specify how large syntactic trees can
be compressed into shorter trees. A ten-fold cross-           To avoid confounding, the judges participated in two
validation evaluation of the classiﬁer yielded an accu-    experiments. In the ﬁrst experiment, they were asked
racy of 87.16% (± 0.14). A majority baseline classi-       to determine on a scale from 1 to 5 how well the systems
ﬁer that chooses the action shift has an accuracy of       did with respect to selecting the most important words
28.72%.                                                    in the original sentence. In the second experiment, they
                                                           were asked to determine on a scale from 1 to 5 how
Employing the decision-based model                         grammatical the outputs were.

To compress sentences, we apply the shift-reduce-drop         We also investigated how sensitive our algorithms are
model in a deterministic fashion. We parse the sentence    with respect to the training data by carrying out the
to be compressed (Collins 1997) and we initialize the      same experiments on sentences of a diﬀerent genre, the
input list with the words in the sentence and the syn-     scientiﬁc one. To this end, we took the ﬁrst sentence of
tactic constituents that “begin” at each word, as shown    the ﬁrst 26 articles made available in 1999 on the cmplg
in Figure 4. We then incrementally inquire the learned     archive. We created a second parallel corpus, which
classiﬁer what action to perform, and we simulate the      we will refer to as the Cmplg Corpus, by generating
execution of that action. The procedure ends when the      by ourselves compressed grammatical versions of these
input list is empty and when the stack contains only       sentences. Since some of the sentences in this corpus
one tree. An inorder traversal of the leaves of this tree  were extremely long, the baseline algorithm could not
produces the compressed version of the sentence given      produce compressed versions in reasonable time.
as input.
                                                              The results in Table 1 show compression rates, and
   Since the model is deterministic, it produces only one  mean and standard deviation results across all judges,
output. The advantage is that the compression is very      for each algorithm and corpus. The results show that
fast: it takes only a few milliseconds per sentence. The   the decision-based algorithm is the most aggressive:
disadvantage is that it does not produce a range of        on average, it compresses sentences to about half of
compressions, from which another system may subse-         their original size. The compressed sentences produced
quently choose. It is straightforward though to extend     by both algorithms are more “grammatical” and con-
the model within a probabilistic framework by applying,    tain more important words than the sentences pro-
for example, the techniques used by Magerman (1995).       duced by the baseline. T -test experiments showed these
                                                           diﬀerences to be statistically signiﬁcant at p < 0.01
                                                           both for individual judges and for average scores across
Original:  Beyond the basic level, the operations of the three products vary widely.

Baseline:  Beyond the basic level, the operations of the three products vary widely.

Noisy-channel: The operations of the three products vary widely.

Decision-based: The operations of the three products vary widely.

Humans:    The operations of the three products vary widely.

Original:  Arborscan is reliable and worked accurately in testing, but it produces very large dxf ﬁles.

Baseline:  Arborscan and worked in, but it very large dxf.

Noisy-channel: Arborscan is reliable and worked accurately in testing, but it produces very large dxf ﬁles.

Decision-based: Arborscan is reliable and worked accurately in testing very large dxf ﬁles.

Humans:    Arborscan produces very large dxf ﬁles.

Original:  Many debugging features, including user-deﬁned break points and variable-watching and

           message-watching windows, have been added.

Baseline:  Debugging, user-deﬁned and variable-watching and message-watching, have been.

Noisy-channel: Many debugging features, including user-deﬁned points and variable-watching and

           message-watching windows, have been added.

Decision-based: Many debugging features.

Humans:    Many debugging features have been added .

                                          Figure 5: Compression examples

Corpus     Avg. orig. sent. length  Compression           Baseline  Noisy-channel            Decision-based   Humans
Test                21 words        Grammaticality
                                    Importance            63.70%        70.37%                    57.19%       53.33%
Cmplg               26 words        Compression          1.78±1.19    4.34±1.02                 4.30±1.33    4.92±0.18
                                    Grammaticality       2.17±0.89    3.38±0.67                 3.54±1.00    4.24 ±0.52
                                    Importance
                                                              –         65.68%                    54.25%       65.68%
                                                              –       4.22±0.99                 3.72±1.53    4.97±0.08
                                                              –       3.42±0.97                 3.24±0.68    4.32±0.54

                                          Table 1: Experimental results

all judges. T -tests showed no signiﬁcant statistical                                  References
diﬀerences between the two algorithms. As Table 1
shows, the performance of the compression algorithms               Barzilay, R.; McKeown, K.; and Elhadad, M. 1999.
is much closer to human performance than baseline per-             Information fusion in the context of multi-document
formance; yet, humans perform statistically better than            summarization. In Proceedings of the 37th Annual
our algorithms at p < 0.01.                                        Meeting of the Association for Computational Linguis-
                                                                   tics (ACL–99), 550–557.
   When applied to sentences of a diﬀerent genre, the
performance of the noisy-channel compression algo-                 Berger, A., and Laﬀerty, J. 1999. Information retrieval
rithm degrades smoothly, while the performance of the              as statistical translation. In Proceedings of the 22nd
decision-based algorithm drops sharply. This is due to             Conference on Research and Development in Informa-
a few sentences in the Cmplg Corpus that the decision-             tion Retrieval (SIGIR–99), 222–229.
based algorithm over-compressed to only two or three
words. We suspect that this problem can be ﬁxed if                 Brown, P.; Della Pietra, S.; Della Pietra, V.; and Mer-
the decision-based compression module is extended in               cer, R. 1993. The mathematics of statistical ma-
the style of Magerman (1995), by computing probabil-               chine translation: Parameter estimation. Computa-
ities across the sequences of decisions that correspond            tional Linguistics 19(2):263–311.
to a compressed sentence. Likewise, there are substan-
tial gains to be had in noisy-channel modeling—we see              Church, K. 1988. A stochastic parts program and noun
clearly in the data many statistical dependencies and              phrase parser for unrestricted text. In Proceedings of
processes that are not captured in our simple initial              the Second Conference on Applied Natural Language
models. More grammatical output will come from tak-                Processing, 136–143.
ing account of subcategory and head-modiﬁer statistics
(in addition to simple word-bigrams), and an expanded              Collins, M. 1997. Three generative, lexicalized mod-
channel model will allow for more tree manipulation                els for statistical parsing. In Proceedings of the 35th
possibilities. Work on extending the algorithms pre-               Annual Meeting of the Association for Computational
sented in this paper to compressing multiple sentences             Linguistics (ACL–97), 16–23.
is currently underway.
                                                                   Grefenstette, G. 1998. Producing intelligent tele-
                                                                   graphic text reduction to provide an audio scanning
                                                                   service for the blind. In Working Notes of the AAAI
Spring Symposium on Intelligent Text Summarization,
111–118.

Jelinek, F. 1997. Statistical Methods for Speech Recog-
nition. The MIT Press.

Jing, H., and McKeown, K. 1999. The decomposition
of human-written summary sentences. In Proceedings
of the 22nd Conference on Research and Development
in Information Retrieval (SIGIR–99).

Knight, K., and Graehl, J. 1998. Machine transliter-
ation. Computational Linguistics 24(4):599–612.

Langkilde, I. 2000. Forest-based statistical sentence
generation. In Proceedings of the 1st Annual Meeting
of the North American Chapter of the Association for
Computational Linguistics.

Linke-Ellis, N. 1999. Closed captioning in Amer-
ica: Looking beyond compliance. In Proceedings of
the TAO Workshop on TV Closed Captions for the
hearing impaired people, 43–59.

Magerman, D. 1995. Statistical decision-tree models
for parsing. In Proceedings of the 33rd Annual Meeting
of the Association for Computational Linguistics, 276–
283.

Mani, I., and Maybury, M., eds. 1999. Advances in
Automatic Text Summarization. The MIT Press.

Mani, I.; Gates, B.; and Bloedorn, E. 1999. Improving
summaries by revising them. In Proceedings of the 37th
Annual Meeting of the Association for Computational
Linguistics, 558–565.

McKeown, K.; Klavans, J.; Hatzivassiloglou, V.;
Barzilay, R.; and Eskin, E. 1999. Towards multidoc-
ument summarization by reformulation: Progress and
prospects. In Proceedings of the Sixteenth National
Conference on Artiﬁcial Intelligence (AAAI–99).

Quinlan, J. 1993. C4.5: Programs for Machine Learn-
ing. San Mateo, CA: Morgan Kaufmann Publishers.

Robert-Ribes, J.; Pfeiﬀer, S.; Ellison, R.; and Burn-
ham, D. 1999. Semi-automatic captioning of TV pro-
grams, an Australian perspective. In Proceedings of
the TAO Workshop on TV Closed Captions for the
hearing impaired people, 87–100.

Witbrock, M., and Mittal, V. 1999. Ultra-
summarization: A statistical approach to generating
highly condensed non-extractive summaries. In Pro-
ceedings of the 22nd International Conference on Re-
search and Development in Information Retrieval (SI-
GIR’99), Poster Session, 315–316.
