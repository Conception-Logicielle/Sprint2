probabilistic_sentence_reduction.txt
Probabilistic Sentence Reduction Using Support Vector Machines
                          As pointed out by Lin (Lin 03), the best sen-                                                       tence reduction output for a single sentence is This paper investigates a novel application of sup-                                                       not approximately best for text summarization. port vector machines (SVMs) for sentence reduction.                                                       This means that “local optimal” refers to the We also propose a new probabilistic sentence reduc-                                                       best reduced output for a single sentence, while tion method based on support vector machine learn-                                                       the best reduced output for the whole text is ing. Experimental results show that the proposed                                                       “global optimal”. Thus, it would be very valu- methods outperform earlier methods in term of sen-                                                       able if the sentence reduction task could gener- tence reduction performance.                                                       ate multiple reduced outputs and select the best                                                       one using the whole text document. However, 1   Introduction                                                       such a sentence reduction method has not yet The most popular methods of sentence reduc-           been proposed. tion for text summarization are corpus based             Support Vector Machines (Vapnik 95), on the methods. Jing (Jing 00) developed a method            other hand, are strong learning methods in com- to remove extraneous phrases from sentences           parison with decision tree learning and other by using multiple sources of knowledge to de-         learning methods (Sholkopf 97). The goal of cide which phrases could be removed. However,         this paper is to illustrate the potential of SVMs while this method exploits a simple model for         for enhancing the accuracy of sentence reduc- sentence reduction by using statistics computed       tion in comparison with previous work. Accord- from a corpus, a better model can be obtained         ingly, we describe a novel deterministic method by using a learning approach.                         for sentence reduction using SVMs and a two-    Knight and Marcu (Knight and Marcu 02)             stage method using pairwise coupling (Hastie proposed a corpus based sentence reduction            98). To solve the problem of generating mul- method using machine learning techniques.             tiple best outputs, we propose a probabilistic They discussed a noisy-channel based approach         sentence reduction model, in which a variant of and a decision tree based approach to sentence        probabilistic SVMs using a two-stage method reduction. Their algorithms provide the best          with pairwise coupling is discussed. way to scale up the full problem of sentence re-         The rest of this paper will be organized as duction using available data. However, these al-      follows: Section 2 introduces the Support Vec- gorithms require that the word order of a given       tor Machines learning. Section 3 describes the sentence and its reduced sentence are the same.       previous work on sentence reduction and our Nguyen and Horiguchi (Nguyen and Horiguchi            deterministic sentence reduction using SVMs. 03) presented a new sentence reduction tech-          We also discuss remaining problems of deter- nique based on a decision tree model without          ministic sentence reduction. Section 4 presents that constraint. They also indicated that se-         a probabilistic sentence reduction method using mantic information is useful for sentence reduc-      support vector machines to solve this problem. tion tasks.                                           Section 5 discusses implementation and our ex-    The major drawback of previous works on            perimental results; Section 6 gives our conclu- sentence reduction is that those methods are          sions and describes some problems that remain likely to output local optimal results, which may     to be solved in the future. have lower accuracy. This problem is caused by the inherent sentence reduction model; that is, only a single reduced sentence can be obtained. 2   Support Vector Machine                                    that consists of sub trees in order to rewrite a Support vector machine (SVM)(Vapnik 95) is a                  small tree. Let RSTACK be a stack that con- technique of machine learning based on statisti-              sists of sub trees which are removed from the cal learning theory. Suppose that we are given                Input list in the rewriting process. l training examples (xi , yi ), (1 ≤ i ≤ l), where xi is a feature vector in n dimensional feature                 • SHIFT action transfers the first word from the space, yi is the class label {-1, +1 } of xi . SVM                Input list into CSTACK. It is written mathe- finds a hyperplane w.x + b = 0 which correctly                    matically and given the label SHIFT. separates the training examples and has a max-                  • REDUCE(lk, X) action pops the lk syntactic imum margin which is the distance between two                     trees located at the top of CSTACK and com- hyperplanes w.x + b ≥ 1 and w.x + b ≤ −1. The                     bines them in a new tree, where lk is an integer optimal hyperplane with maximum margin can                        and X is a grammar symbol. be obtained by solving the following quadratic programming.                                                    • DROP X action moves subsequences of words                                                                   that correspond to syntactic constituents from                        1                                      P                                      l                                                                   the Input list to RSTACK.            min         2 kwk + C0         ξi                                       i                 (1)     • ASSIGN TYPE X action changes the label of            s.t.   yi (w.xi + b) ≥ 1 − ξi                                                                   trees at the top of the CSTACK. These POS            ξi ≥ 0                                                                   tags might be different from the POS tags in  where C0 is the constant and ξi is a slack vari-                 the original sentence. able for the non-separable case. In SVM, the                    • RESTORE X action takes the X element in optimal hyperplane is formulated as follows:                      RSTACK and moves it into the Input list,                        Ã l                          !             where X is a subtree.                         X         f (x) = sign          αi yi K(xi , x) + b       (2)                           1                                   For convenience, let configuration be a status                                                               of Input list, CSTACK and RSTACK. Let cur-   where αi is the Lagrange multiple, and                      rent context be the important information in a K(x0 , x00 ) is a kernel function, the SVM calcu-             configuration. The important information are lates similarity between two arguments x0 and x00 . For instance, the Polynomial kernel func-               defined as a vector of features using heuristic tion is formulated as follow:                                 methods as in (Knight and Marcu 02), (Nguyen                                                               and Horiguchi 03).                  K(x0 , x00 ) = (x0 .x00 )p             (3)      The main idea behind deterministic sentence                                                               reduction is that it uses a rule in the current  SVMs estimate the label of an unknown ex-                                                               context of the initial configuration to select a ample x whether the sign of f (x) is positive or                                                               distinct action in order to rewrite an input sen- not.                                                               tence into a reduced sentence. After that, the 3   Deterministic Sentence Reduction                          current context is changed to a new context and     Using SVMs                                                the rewriting process is repeated for selecting                                                               an action that corresponds to the new context. 3.1 Problem Description                                                               The rewriting process is finished when it meets In the corpus-based decision tree approach, a                 a termination condition. Here, one rule corre- given input sentence is parsed into a syntax tree             sponds to the function that maps the current and the syntax tree is then transformed into a                context to a rewriting action. These rules are small tree to obtain a reduced sentence.                      learned automatically from the corpus of long    Let t and s be syntax trees of the original sen-           sentences and their reduced sentences (Knight tence and a reduced sentence, respectively. The               and Marcu 02), (Nguyen and Horiguchi 03). process of transforming syntax tree t to small tree s is called “rewriting process” (Knight and              3.2 Example Marcu 02), (Nguyen and Horiguchi 03). To                      Figure 1 shows an example of applying a se- transform the syntax tree t to the syntax tree                quence of actions to rewrite the input sentence s, some terms and five rewriting actions are de-              (a, b, c, d, e), when each character is a word. It fined.                                                        illustrates the structure of the Input list, two    An Input list consists of a sequence of words              stacks, and the term of a rewriting process based subsumed by the tree t where each word in the                 on the actions mentioned above. For example, Input list is labelled with the name of all syntac-           in the first row, DROP H deletes the sub-tree tic constituents in t. Let CSTACK be a stack                  with its root node H in the Input list and stores it in the RSTACK. The reduced tree s can be obtained after applying a sequence of actions as follows: DROP H; SHIFT; ASSIGN TYPE K; DROP B; SHIFT; ASSIGN TYPE H; REDUCE 2 F; RESTORE H; SHIFT; ASSIGN TYPE D; RE- DUCE 2G. In this example, the reduced sentence is (b, e, a).
