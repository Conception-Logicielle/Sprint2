marcu_statistics_sentence_pass_one.txt
From: AAAI-00 Proceedings. Copyright © 2000, AAAI (www.aaai.org). All rights reserved.
                                     rules for deleting information that is redundant, com-                                                                          pressing long sentences into shorter ones, aggregating    When humans produce summaries of documents, they                                                                          sentences, repairing reference links, etc.    do not simply extract sentences and concatenate them.    Rather, they create new sentences that are grammati-                     Our goal is also to generate coherent abstracts. How-    cal, that cohere with one another, and that capture the               ever, in contrast with the above work, we intend to    most salient pieces of information in the original doc-               eventually use Abstract, Text tuples, which are widely    ument. Given that large collections of text/abstract                  available, in order to automatically learn how to rewrite    pairs are available online, it is now possible to envision            Texts as coherent Abstracts. In the spirit of the work    algorithms that are trained to mimic this process. In                 in the statistical MT community, which is focused on    this paper, we focus on sentence compression, a sim-                  sentence-to-sentence translations, we also decided to fo-    pler version of this larger challenge. We aim to achieve              cus ﬁrst on a simpler problem, that of sentence compres-    two goals simultaneously: our compressions should be                  sion. We chose this problem for two reasons:    grammatical, and they should retain the most impor-    tant pieces of information. These two goals can con-                  • First, the problem is complex enough to require the    ﬂict. We devise both noisy-channel and decision-tree                    development of sophisticated compression models:    approaches to the problem, and we evaluate results                      Determining what is important in a sentence and    against manual compressions and a simple baseline.                      determining how to convey the important informa-                                                                            tion grammatically, using only a few words, is just a                        Introduction                                        scaled down version of the text summarization prob-  Most of the research in automatic summarization                           lem. Yet, the problem is simple enough, since we do  has focused on extraction, i.e., on identifying the                       not have to worry yet about discourse related issues,  most important clauses/sentences/paragraphs in texts                      such as coherence, anaphors, etc.  (see (Mani & Maybury 1999) for a representative col-                    • Second, an adequate solution to this problem has  lection of papers). However, determining the most im-                     an immediate impact on several applications. For  portant textual segments is only half of what a summa-                    example, due to time and space constraints, the  rization system needs to do because, in most cases, the                   generation of TV captions often requires only the  simple catenation of textual segments does not yield                      most important parts of sentences to be shown on a  coherent outputs. Recently, a number of researchers                       screen (Linke-Ellis 1999; Robert-Ribes et al. 1999).  have started to address the problem of generating co-                     A good sentence compression module would there-  herent summaries: McKeown et al. (1999), Barzilay et                      fore have an impact on the task of automatic cap-  al. (1999), and Jing and McKeown (1999) in the context                    tion generation. A sentence compression module  of multidocument summarization; Mani et al. (1999) in                     can also be used to provide audio scanning ser-  the context of revising single document extracts; and                     vices for the blind (Grefenstette 1998). In gen-  Witbrock and Mittal (1999) in the context of headline                     eral, since all systems aimed at producing coher-  generation.                                                               ent abstracts implement manually written sets of     The approach proposed by Witbrock and Mit-                             sentence compression rules (McKeown et al. 1999;  tal (1999) is the only one that applies a probabilistic                   Mani, Gates, & Bloedorn 1999; Barzilay, McKeown,  model trained directly on Headline, Document pairs.                     & Elhadad 1999), it is likely that a good sentence  However, this model has yet to scale up to generat-                       compression module would impact the overall quality  ing multiple-sentence abstracts as well as well-formed,                   of these systems as well. This becomes particularly  grammatical sentences. All other approaches employ                        important for text genres that use long sentences.  sets of manually written or semi-automatically derived                                                                            In this paper, we present two approaches to the sen-  Copyright  c 2000, American Association for Artiﬁcial In-              tence compression problem. Both take as input a se-  telligence (www.aaai.org). All rights reserved.                         quence of words W = w1 , w2 , . . . , wn (one sentence). An algorithm may drop any subset of these words. The             It is advantageous to break the problem down this words that remain (order unchanged) form a compres-           way, as it decouples the somewhat independent goals sion. There are 2n compressions to choose from—some           of creating a short text that (1) looks grammatical, are reasonable, most are not. Our ﬁrst approach de-           and (2) preserves important information. It is easier to velops a probabilistic noisy-channel model for sentence       build a channel model that focuses exclusively on the compression. The second approach develops a decision-         latter, without having to worry about the former. That based, deterministic model.                                   is, we can specify that a certain substring may represent                                                               unimportant information, but we do not need to worry    A noisy-channel model for sentence                         that deleting it will result in an ungrammatical struc-               compression                                     ture. We leave that to the source model, which worries                                                               exclusively about well-formedness. In fact, we can make This section describes a probabilistic approach to the        use of extensive prior work in source language modeling compression problem. In particular, we adopt the noisy        for speech recognition, machine translation, and natu- channel framework that has been relatively successful in      ral language generation. The same goes for actual com- a number of other NLP applications, including speech          pression (“decoding” in noisy-channel jargon)—we can recognition (Jelinek 1997), machine translation (Brown        re-use generic software packages to solve problems in all et al. 1993), part-of-speech tagging (Church 1988),           these application domains. transliteration (Knight & Graehl 1998), and informa- tion retrieval (Berger & Laﬀerty 1999).                       Statistical Models    In this framework, we look at a long string and imag- ine that (1) it was originally a short string, and then       In the experiments we report here, we build very sim- (2) someone added some additional, optional text to it.       ple source and channel models. In a departure from Compression is a matter of identifying the original short     the above discussion and from previous work on statis- string. It is not critical whether or not the “original”      tical channel models, we assign probabilities Ptree (s) string is real or hypothetical. For example, in statistical   and Pexpand tree (t | s) to trees rather than strings. In machine translation, we look at a French string and say,      decoding a new string, we ﬁrst parse it into a large tree t “This was originally English, but someone added ‘noise’       (using Collins’ parser (1997)), and we then hypothesize to it.” The French may or may not have been translated        and rank various small trees. from English originally, but by removing the noise, we           Good source strings are ones that have both (1) a can hypothesize an English source—and thereby trans-          normal-looking parse tree, and (2) normal-looking word late the string. In the case of compression, the noise        pairs. Ptree (s) is a combination of a standard proba- consists of optional text material that pads out the core     bilistic context-free grammar (PCFG) score, which is signal. For the larger case of text summarization, it may     computed over the grammar rules that yielded the tree be useful to imagine a scenario in which a news editor        s, and a standard word-bigram score, which is com- composes a short document, hands it to a reporter, and        puted over the leaves of the tree. For example, the tells the reporter to “ﬂesh it out” . . . which results in    tree s =(S (NP John) (VP (VB saw) (NP Mary))) is the article we read in the newspaper. As summarizers,         assigned a score based on these factors: we may not have access to the editor’s original version (which may or may not exist), but we can guess at it—            Ptree (s) = P(TOP → S | TOP) · which is where probabilities come in.                            P(S → NP VP | S) · P(NP → John | NP) ·    As in any noisy channel application, we must solve            P(VP → VB NP | VP) · P(VP → saw | VB) · three problems:                                                  P(NP → Mary | NP) ·                                                                  P(John | EOS) · P(saw | John) · • Source model. We must assign to every string s a               P(Mary | saw) · P(EOS | Mary)   probability P(s), which gives the chance that s is gen-   erated as an “original short string” in the above hy-          Our stochastic channel model performs minimal op-   pothetical process. For example, we may want P(s)           erations on a small tree s to create a larger tree t. For   to be very low if s is ungrammatical.                       each internal node in s, we probabilistically choose an                                                               expansion template based on the labels of the node and • Channel model. We assign to every pair of strings           its children. For example, when processing the S node   s, t a probability P(t | s), which gives the chance       in the tree above, we may wish to add a prepositional   that when the short string s is expanded, the result        phrase as a third child. We do this with probability   is the long string t. For example, if t is the same         P(S → NP VP PP | S → NP VP). Or we may choose   as s except for the extra word “not,” then we may           to leave it alone, with probability P(S → NP VP | S →   want P(t | s) to be very low. The word “not” is not         NP VP). After we choose an expansion template, then   optional, additional material.                              for each new child node introduced (if any), we grow a • Decoder. When we observe a long string t, we search         new subtree rooted at that node—for example (PP (P   for the short string s that maximizes P(s | t). This        in) (NP Pittsburgh)). Any particular subtree is grown   is equivalent to searching for the s that maximizes         with probability given by its PCFG factorization, as   P(s) · P (t | s).                                           above (no bigrams).               G                          G                           G                The documentation is typical of Epson quality: excellent.          H               A           H           A                                                               F           D           Documentation is excellent.          a     C         B       D   a       C       D                    e                b Q           R   e                                                           H         K                 All of our design goals were achieved and the delivered                                              b       e              b                    Z         d                                                           a                           performance matches the speed of the underlying device.                    c                                                                                       All design goals were achieved.                    (t)                   (s1)                     (s2)                Reach’s E-mail product, MailMan, is a message- manage-                                                                                       ment system designed initially for VINES LANs that will              Figure 1: Examples of parse trees.                                       eventually be operating system-independent.                                                                                       MailMan will eventually be operating system-independent.                                                                                       Although the modules themselves may be physically and/or                                                                                       electrically incompatible, the cable-speciﬁc jacks on them Example                                                                               provide industry-standard connections.                                                                                       Cable-speciﬁc jacks provide industry-standard connections. In this section, we show how to tell whether one poten-                               Ingres/Star prices start at $2,100. tial compression is more likely than another, according                               Ingres/Star prices start at $2,100. to the statistical models described above. Suppose we observe the tree t in Figure 1, which spans the string abcde. Consider the compression s1, which is shown in                                      Figure 2: Examples from our parallel corpus. the same ﬁgure.   We     compute       the factors Ptree (s1) and                                         P(G → H A | G → H A) Pexpand tree (t | s1). Breaking this down further,                                        P(A → C B D | A → C B D) the source PCFG and word-bigram factors, which                                                                                            P(B → Q R | B → Q R) describe Ptree (s1), are:                                                                                            P(Q → Z | Q → Z)    P(TOP → G | TOP)                       P(H → a | H)    P(G → H A | G)                         P(C → b | C)                                   Now we can simply compare Pexpand tree (s1 |    P(A → C D | A)                         P(D → e | D)                                t) = Ptree (s1) · Pexpand tree (t | s1))/Ptree (t) ver-                                                                                       sus Pexpand tree (t | t) = Ptree (t) · Pexpand tree (t |    P(a | EOS)                             P(e | b)    P(b | a)                               P(EOS | e)                                  t))/Ptree (t) and select the more likely one. Note that                                                                                       Ptree (t) and all the PCFG factors can be canceled out,                                                                                       as they appear in any potential compression. Therefore, The channel expansion-template factors and the chan-                                  we need only compare compressions of the basis of the nel PCFG (new tree growth) factors, which describe                                    expansion-template probabilities and the word-bigram Pexpand tree (t | s1), are:                                                           probabilities. The quantities that diﬀer between the                                                                                       two proposed compressions are boxed above. There-    P(G → H A | G → H A)                                                               fore, s1 will be preferred over t if and only if:    P(A → C B D | A → C D)                                                                 P(e | b) · P(A → C B D | A → C D) >    P(B → Q R | B)                                        P(Z → c | Z)                     P(b | a) · P(c | b) · P(d | c) ·    P(Q → Z | Q)                                          P(R → d | R)                     P(A → C B D | A → C B D) ·                                                                                           P(B → Q R | B → Q R) · P(Q → Z | Q → Z)    A diﬀerent compression will be scored with a diﬀerent set of factors. For example, consider a compression of                                Training Corpus t that leaves t completely untouched. In that case, the                               In order to train our system, we used the Ziﬀ-Davis source costs Ptree (t) are:                                                           corpus, a collection of newspaper articles announcing                                                                                       computer products. Many of the articles in the corpus    P(TOP → G | TOP)                       P(H → a | H)                   P(a | EOS)   are paired with human written abstracts. We automat-    P(G → H A | G)                         P(C → b | C)                   P(b | a)     ically extracted from the corpus a set of 1067 sentence                                                                                       pairs. Each pair consisted of a sentence t = t1 , t2 , . . . , tn    P(A → C D | A)                         P(Z → c | Z)                   P(c | b)     that occurred in the article and a possibly compressed    P(B → Q R | B)                         P(R → d | R)                   P(d | c)     version of it s = s1 , s2 , . . . , sm , which occurred in the    P(Q → Z | Q)                           P(D → e | D)                   P(e | d)     human written abstract. Figure 2 shows a few sentence                                                                          P(EOS | e)   pairs extracted from the corpus.                                                                                          We decided to use such a corpus because it is con-                                                                                       sistent with two desiderata speciﬁc to summarization The channel costs Pexpand tree (t | t) are:                                           work: (i) the human-written Abstract sentences are grammatical; (ii) the Abstract sentences represent in a     semantic representation into a vast number of potential compressed form the salient points of the original news-    English renderings. These renderings are packed into paper Sentences. We decided to keep in the corpus un-       a forest, from which the most promising sentences are compressed sentences as well, since we want to learn        extracted using statistical scoring. not only how to compress a sentence, but also when to         For our purposes, the extractor selects the trees with do it.                                                      the best combination of word-bigram and expansion-                                                             template scores. It returns a list of such trees, one for Learning Model Parameters                                   each possible compression length. For example, for We collect expansion-template probabilities from our        the sentence Beyond that basic level, the operations of parallel corpus. We ﬁrst parse both sides of the parallel   the three products vary, we obtain the following “best” corpus, and then we identify corresponding syntactic        compressions, with negative log-probabilities shown in nodes. For example, the parse tree for one sentence         parentheses (smaller = more likely): may begin (S (NP . . . ) (VP . . . ) (PP . . . )) while the parse tree for its compressed version may begin (S      Beyond that basic level, the operations of the three products vary (NP . . . ) (VP . . . )). If these two S nodes are deemed   widely (1514588) to correspond, then we chalk up one joint event (S →        Beyond that level, the operations of the three products vary widely NP VP, S → NP VP PP); afterwards we normalize.              (1430374) Not all nodes have corresponding partners; some non-        Beyond that basic level, the operations of the three products vary correspondences are due to incorrect parses, while oth-     (1333437) ers are due to legitimate reformulations that are beyond    Beyond that level, the operations of the three products vary the scope of our simple channel model. We use standard      (1249223) methods to estimate word-bigram probabilities.              Beyond that basic level, the operations of the products vary                                                             (1181377) Decoding                                                    The operations of the three products vary widely (939912) There is a vast number of potential compressions of a       The operations of the products vary widely (872066) large tree t, but we can pack them all eﬃciently into a     The operations of the products vary (748761) shared-forest structure. For each node of t that has n      The operations of products vary (690915) children, we                                                Operations of products vary (809158) • generate 2n − 1 new nodes, one for each non-empty         The operations vary (522402)    subset of the children, and                              Operations vary (662642)
