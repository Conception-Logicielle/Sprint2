Stolcke_1996_Automatic_linguistic.txt
                             AUTOMATIC LINGUISTIC SEGMENTATION
                                        could use the knowledge incorporated in an automatic segmenter                                                                           to help end-point a userâ€™s speech input. A speech indexing and re- As speech recognition moves toward more unconstrained domains                                                                           trieval system (such as for transcribed broadcast audio) could pro- such as conversational speech, we encounter a need to be able to                                                                           cess its data in more meaningful units if the locations of linguistic segment (or resegment) waveforms and recognizer output into lin-                                                                           segment boundaries were known. guistically meaningful units, such a sentences. Toward this end, we present a simple automatic segmenter of transcripts based on           Our main motivation for the work reported here comes from speech N-gram language modeling. We also study the relevance of sev-             language modeling. Experiments at the 1995 Johns Hopkins Lan- eral word-level features for segmentation performance. Using only         guage Modeling Workshop showed that the quality of a language word-level information, we achieve 85% recall and 70% precision           model (LM) can be improved if both training and test data are seg- on linguistic boundary detection.                                         mented linguistically, rather than acoustically [8]. We showed in                                                                           [10] and [9] that proper modeling of filled pauses requires knowl-                   1.    INTRODUCTION                                      edge of linguistic segment boundaries. We found for example that Todayâ€™s large-vocabulary speech recognizers typically prefer to pro-      segment-internal filled pauses condition the following words quite cess a few tens of seconds of speech at a time, to keep the time and      differently from segment-initial filled pauses. Finally, recent efforts memory demands of the decoder within bounds. For longer inputs,           in language modeling for conversational speech, such as [8], attempt the waveform is usually presegmented into shorter pieces based on         to capitalize on the internal structure of utterances and turns. Such simple acoustic criteria, such as nonspeech intervals (e.g., pauses)      models are formulated in terms of linguistic units and therefore re- and turn boundaries (when several speakers are involved). We refer        quire linguistic segmentations to be applicable. to such segmentations as acoustic segmentations.                                                                                                   3.    METHOD Acoustic segmentations generally do not reflect the linguistic struc-                                                                           Our main goal for this work was to examine to what extent various ture of utterances. They may fragment sentences or semantic units,                                                                           kinds of lexical (word-based) information were useful for automatic or group together spans of unrelated units. We examine several rea-                                                                           linguistic segmentation. This precluded a study based on the out- sons why such behavior is undesirable, and propose that linguistic                                                                           put of existing speech recognition systems, which currently achieve segmentations be used instead. This requires algorithms for auto-                                                                           about 40-50% word error rate on the type of data used in our exper- matically finding linguistic units. In this paper we report on first                                                                           iments. At such high error rates, the analysis of any segmentation results from our ongoing efforts toward such an automatic linguis-                                                                           algorithm and the features it uses would likely be confounded by tic segmentation. In all further discussion, unless otherwise noted,                                                                           the unreliable nature of the input data. We therefore chose to elimi- the terms â€˜segment,â€™ â€˜segmentation,â€™ etc. will refer to linguistic seg-                                                                           nate the problem of inaccurate speech recognition and tested our al- mentations.                                                                           gorithms on hand-transcribed word-level transcripts of spontaneous   2. THE IMPORTANCE OF LINGUISTIC                                         speech from the Switchboard corpus [4]. An additional benefit of                                                                           this approach is that the models employed by the segmentation al-             SEGMENTATION                                                  gorithms can also be directly used as language models for speech Acoustic segmentations are inadequate in cases where the output           recognizers for the same type of data, an application we are pursu- of a speech recognizer is to serve as input for further processing        ing as well. based on syntactically or semantically coherent units. This includes                                                                           The segmentation approaches we investigated all fell within the fol- most natural language (NL) parsers or NL understanding or transla-                                                                           lowing framework. We first trained a statistical language model tion systems. For such systems, the fragmented recognition output                                                                           of the N-gram variety to model the distribution of both words and would have to be put back together and large spans of unrelated                                                                           segment boundaries. (For this purpose, segment boundaries were material would need to be resegmented into linguistic units.                                                                           represented as special tokens <s> within the text.) The segmenta- Automatic detection of linguistic segments could also improve the         tion information was removed from the test data, and the language user interface of many speech systems. A spoken language system           model was used to hypothesize the most probable locations of seg- ment boundaries. The resulting segmentations were then evaluated             computation yields the likelihoods of the states at each position k: along a number of metrics. As training data, we used 1.4 million words of Switchboard tran-                    PNO-S (w1 : : : wk )    =   PNO-S (w1 : : : wk 1 )  scripts annotated for linguistic segmentations by the UPenn Tree-                                                  p(wkjwk 2 wk 1 ) bank project [7], comprising a total of 193,000 segments. One half of the standard Switchboard development test set, totaling 10,000                                                                                                                 +PS (w1 : : : wk 1 )    words and 1,300 segments, was used for testing.                                                                    p(wkj<s>wk 1 ) The hand-annotated segments encompassed different kinds of lin-                         PS (w1 : : : wk )   =   PNO-S (w1 : : : wk 1 )  guistic structures, including                                                                                      p(<s>jwk 2 wk 1 ) p(wkj<s>)                                                                                                                 +PS (w1 : : : wk 1 )        Complete sentences                                                                                            p(<s>j<s>wk 1 ) p(wkj<s>)     Stand-alone phrases     Disfluent sentences aborted in mid-utterance1     Interjections and back-channel responses                                A corresponding Viterbi algorithm is used to find the most likely                                                                              sequence of S and NO-S (i.e., a segmentation) for a given word The following excerpt illustrates the character of the data. Linguis-        string. This language model is a full implementation of the model tic segment boundaries are marked <s>, whereas acoustic segmen-              approximated in [8]. The hidden disfluency model of [10] has a tations are indicated by //.                                                 similar structure. As indicated in the formulae above, we currently                                                                              use at most two words of history in the local conditional probabili-       B.44: Worried that they're not going to                                ties p(j). Longer N-grams can be used if more state information is       get enough attention? <s> //                                           kept.       A.45: Yeah, <s> and, uh, you know, colds                               The local N-gram probabilities are estimated from the training data       and things like that <laughter> get -- //                              by using Katz backoff with Good-Turing discounting [6].       B.46:     Yeah.     <s> //                                                                                                             5. RESULTS       A.47: -- spread real easy and things,       <s> but, // and they're expensive <s> and,                             5.1.      Baseline Segmentation Model       // <lipsmack> // course, // there's a lot       of different types of day care available,                                                                              The first model we looked at models only plain words and segment       too, // you know, where they teach them                                                                              boundaries in the manner described. It was applied to the concate-       academic things. <s> //                                                                              nation of all turns of a conversation side, with no additional con-                                                                              textual cues supplied. During testing, this model thus operates with       B.48:     Yes.    <s> //                                                                              very minimal information, i.e., with only the raw word sequence to                                                                              be segmented. Table 1 shows results for bigram and trigram mod- This short transcript shows some of the ubiquitous features of spon-                                                                              els. The performance metrics used are defined as follows. Recall taneous speech affecting segmentation, such as
