Hybrid Headlines: Combining Topics and Sentence Compression

David Zajic, Bonnie Dorr, Stacy President                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Richard Schwartz

Department of Computer Science                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         BBN Technologies

University of Maryland                                   9861 Broken Land Parkway, Suite 156

College Park, MD 20742                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Columbia, MD 21046

dmzajic,bonnie @umiacs.umd.edu                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         schwartz@bbn.com
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ¡

stacypre@cs.umd.edu

                      Abstract                           sets of topics with speciﬁc documents. The top-
                                                         ics and sentence compressions are combined in a
      This paper presents Topiary, a headline-           manner that preserves the advantages of each ap-
      generation system that creates very                proach: the ﬂuency and event-oriented informa-
      short, informative summaries for news              tion from the lead sentence with the broader cov-
      stories by combining sentence compres-             erage of the topic models.
      sion and unsupervised topic discovery.
      We will show that the combination of                  The next section presents previous work in the
      linguistically motivated sentence com-             area of automatic summarization. Following this
      pression with statistically selected topic         we describe Hedge Trimmer and Unsupervised
      terms performs better than either alone,           Topic Discovery in more detail, and describe the
      according to some automatic summary                algorithm for combining sentence compression
      evaluation measures. In addition we de-            with topics. Next we show that Topiary scores
      scribe experimental results establishing           higher than either Hedge Trimmer or Unsuper-
      an appropriate extrinsic task on which to          vised Topic Discovery alone according to certain
      measure the effect of summarization on             automatic evaluation tools for summarization. Fi-
      human performance. We demonstrate                  nally we propose event tracking as an extrinsic
      the usefulness of headlines in compar-             task using automatic summarization for measur-
      ison to full texts in the context of this          ing how human performance is affected by auto-
      extrinsic task.                                    matic summarization, and for correlating human
                                                         peformance with automatic evaluation tools. We
1 Introduction                                           describe an experiment that supports event track-
                                                         ing as an appropriate task for this purpose, and
In this paper we present Topiary, a headline-            show results that suggest that a well-written hu-
generation system that creates very short, infor-        man headline is nearly as useful for event tracking
mative summaries for news stories by combining           as the full text.
sentence compression and unsupervised topic dis-
covery. Hedge Trimmer performs sentence com-             2 Previous Work
pression by removing constituents from a parse
tree of the lead sentence according to a set of          Hedge Trimmer is a sentence compression algo-
linguistically-motivated heuristics until a length       rithm based on linguistically-motivated heuristics.
threshold is reached. Unsupervised Topic Discov-         Previous work on sentence compression (Knight
ery is a statistical method for deriving a set of topic  and Marcu, 2000) uses a noisy-channel model to
models from a document corpus, assigning mean-           ﬁnd the most probable short string that gener-
ingful names to the topic models, and associating        ated the observed full sentence. Other work (Eu-
                                                         ler, 2002) combines a word-list with syntactic in-
formation to decide which words and phrases to         parsing the sentence using the BBN SIFT parser
cancel. Our approach differs from Knight’s in          (Miller et al., 1998) and removing low-content
that we do not use a statistical model, so we do       syntactic constituents. Some constituents, such as
not require any prior training on a large corpus       certain determiners (the, a) and time expressions
of story/headline pairs. Topiary shares with Eu-       are always removed, because they rarely occur in
ler the combination of topic lists and sentence        human-generated headlines and are low-content
compression. However Euler uses the topic lists        in comparison to other constituents. Other con-
to guide sentence selection and compression to-        stituents are removed one-by-one until a length
wards a query-speciﬁc summary, whereas Topiary         threshold has been reached. These include, among
uses topics to augment the concept coverage of a       others, relative clauses, verb-phrase conjunction,
generic summary.                                       preposed adjuncts and prepositional phrases that
                                                       do not contain named entities. 1 The threshold can
   Summaries can also consist of lists of words or     be speciﬁed either in number of words or number
short phrases indicating that the topic or concept     of characters. If the threshold is speciﬁed in num-
they denote is important in the document. Extrac-      ber of characters, Hedge Trimmer will not include
tive topic summaries consist of keywords or key        partial words.
phrases that occur in the document. (Bergler et al.,
2003) achieves this by choosing noun phrases that      3.2 Recent Hedge Trimmer Work
represent the most important text entities, as repre-
sented by noun phrase coreference chains. (Zhou        Recently we have investigated a rendering of the
and Hovy, 2003) imposes ﬂuency onto a topic list       summary as “Headlinese” (Ma˚rdh, 1980) in which
by ﬁnding phrase clusters early in the text that con-  certain constituents are dropped with no loss of
tain important topic words found throughout the        meaning. The result of this investigation has been
text. In text categorization documents are assigned    used to enhance Hedge Trimmer, most notably
to pre-deﬁned categories. This is equivalent to as-    the removal of certain instances of have and be.
signing topics to a document from a static topic       For example, the previous headline generator pro-
list, so the words in the summary need not actually    duced summaries such as Sentence (2), whereas
appear in the document. (Lewis, 1992) describes        the have/be removal produces (3).
a probabilistic feature-based method for assigning
Reuters topics to news stories. OnTopic (Schwartz      (1) Input: The senior Olympic ofﬁcial who lev-
et al., 1997) uses a HMM to assign topics from a             eled stunning allegations of corruption within
topic-annotated corpus to a new document.                    the IOC said Sunday he had been “muzzled”
                                                             by president Juan Antonio Samaranch and
3 Algorithm Description                                      might be thrown out of the organization.

Topiary produces headlines by combining the out-       (2) Without participle have/be removal: Senior
put of Hedge Trimmer, a sentence compression                 Olympic ofﬁcial said he had been muzzled
algorithm, with Unsupervised Topic Detection
(UTD). In this section we will give brief descrip-     (3) With participle have/be removal: Senior
tions of Hedge Trimmer, recent modiﬁcations to               Olympic ofﬁcial said he muzzled by presi-
Hedge Trimmer, and UTD. We will then describe                dent Juan Antonio Samaranch
how Hedge Trimmer and UTD are combined.
                                                       Have and be are removed if they are part of a past
3.1 Hedge Trimmer                                      or present participle construction. In this exam-
                                                       ple, the removal of had been allows a high-content
Hedge Trimmer (Dorr et al., 2003b) generates           constituent by president Juan Antonio Samaranch
a headline for a news story by compressing the         to ﬁt into the headline.
lead (or main) topic sentence according to a lin-
guistically motivated algorithm. For news stories,        The removal of forms of to be allows Hedge
the ﬁrst sentence of the document is taken to be       Trimmer to produce headlines that concentrate
the lead sentence. The compression consists of
                                                           1More details of the Hedge Trimmer algorithm can be
                                                       found in (Dorr et al., 2003b) and (Dorr et al., 2003a).
more information in the allowed space. The re-        emergency shortening methods which are only
moval of forms of to be results in sentences that     to be used when the alternative is truncating the
are not grammatical in general English, but are       headline after the threshold, possibly cutting the
typical of Headlinese English. For example, sen-      middle of a word. These include removal of ad-
tences (5), (6) and all other examples in this paper  verbs and adverbial phrases, adjectives and adjec-
were trimmed to ﬁt in 75 characters.                  tive phrases, and nouns that modify other nouns.

(4) Input: Leading maxi yachts Brindabella, Say-      3.3 Unsupervised Topic Discovery
      onara and Marchioness were locked in a
      three-way duel down the New South Wales         Unsupervised Topic Discovery (UTD) is used
      state coast Saturday as the Sydney to Hobart    when we do not have a corpus annotated with top-
      ﬂeet faced deteriorating weather.               ics. It takes as input a large unannotated corpus
                                                      in any language and automatically creates a set of
(5) Without to be removal: Sayonara and Mar-          topic models with meaningful names. The algo-
      chioness were locked in three                   rithm has several stages. First, it analyzes the cor-
                                                      pus to ﬁnd strings of words that occur frequently.
(6) With to be removal: Leading maxi yachts           (It does this using a Minimum Description Length
      Brindabella Sayonara and Marchioness            criterion.) These are frequently phrases that are
      locked in three                                 meaningful names of topics.

   When have and be occur with a modal verb, the         Second, it ﬁnds the high-content words in each
modal verb is also removed. Sentence (9) shows        document (using a modiﬁed tf.idf measure). These
an example of this. It could be argued that by        are possible topic names for each document. It
removing modals such as should and would the          keeps only those names that occur in at least four
meaning is vitally changed. The intended use of       different documents. These are taken to be an ini-
the headline must be considered. If the headlines     tial set of topic names.
are to be used for determining query relevance, re-
moval of modals may not hinder the user while            In the third stage UTD trains topic models cor-
making room for additional high-content words         responding to these topic names. The modiﬁed
may help.                                             EM procedure of OnTopicTMis used to determine
                                                      which words in the documents often signify these
(7) Input: Organizers of December’s Asian             topic names. This produces topic models.
      Games have dismissed press reports that a
      sports complex would not be completed on           Fourth, these topic models are used to ﬁnd the
      time, saying preparations are well in hand, a   most likely topics for each document. This often
      local newspaper said Friday.                    adds new topics to documents, even though the
                                                      topic name did not appear in the document.
(8) Without Modal-Have/Be Removal: Organiz-
      ers have dismissed press reports saying            We found, in various experiments, that the top-
                                                      ics derived by this procedure were usually mean-
(9) With Modal-Have/Be Removal: Organizers            ingful and that the topic assignment was about as
      dismissed press reports sports complex not      good as when the topics were derived from a cor-
      completed saying                                pus that was annotated by people. We have also
                                                      used this procedure on different languages and
   In addition when it or there appears as a subject  shown the same behavior.
with a form of be or have, as in extraposition (It
was clear that the thief was hungry) or existential      Sentence (10) is a topic list generated for a story
clauses (There have been a spate of dog maulings),    about the investigation into the bombing of the
the subject and the verb are removed.                 U.S. Embassy in Nairobi on August 7, 1998.

   Finally, for situations in which the length        (10) BIN LADEN EMBASSY BOMBING PO-
threshold is a hard constraint, we added some               LICE OFFICIALS PRISON HOUSE FIRE
                                                            KABILA
3.4 Combination of Hedge Trimmer and                 document no more than than 75 characters. The
      Topics: Topiary                                different ROUGE variants are sorted by overall
                                                     performance of the systems. The key observations
The Hedge Trimmer algorithm is constrained to        are that there was a wide range of performance
take its headline from a single sentence. It is of-  among the submitted systems, and that Topiary
ten the case that there is no single sentence that   scored ﬁrst or second among the automatic sys-
contains all the important information in a story.   tems on each ROUGE measure.
The information can be spread over two or three
sentences, with pronouns or ellipsis used to link    4 Evaluation
them. In addition, our algorithms do not always
select the ideal sentence and trim it perfectly.     We used two automatic evaluation systems, BLEU
                                                     (Papineni et al., 2002) and ROUGE (Lin and
   Topics alone also have drawbacks. UTD rarely      Hovy, 2003), to evaluate nine variants of our head-
generates any topic names that are verbs. Thus       line generation systems. Both measures make n-
topic lists are good at indicating the general sub-  gram comparisons of the candidate systems to a
ject are but rarely give any direct indication of    set of reference summaries. In our evaluations
what events took place.                              four reference summaries for each document were
                                                     used. The nine variants were run on 489 stories
   Topiary is a modiﬁcation of the enhanced          from the DUC2004 single-document summariza-
Hedge Trimmer algorithm to take a list of top-       tion headline generation task. The threshold was
ics with relevance scores as additional input. The   75 characters, and longer headlines were truncated
compression threshold is lowered so that there       to 75 characters. We also evaluated a baseline
will be room for the highest scoring topic term      that consisted of the ﬁrst 75 characters of the doc-
that isn’t already in the headline. This amount of   ument. The systems and the average lengths of
threshold lowering is dynamic, because the trim-     the headlines they produced are shown in Table
ming of the sentence can remove a previously in-     1. Trimmer headlines tend to be shorter than the
eligible high-scoring topic term from the headline.  threshold because Trimmer removes constituents
After trimming is complete, additional topic terms   until the length is below the threshold. Sometimes
that do not occur in the headline are added to use   it must remove a large constituent in order to get
up any remaining space.                              below the threshold. Topiary is able to make full
                                                     use of the space by ﬁlling in topic words.
   This often results in one or more main topics
about the story and a short sentence that says what  4.1 ROUGE
happened concerning them. The combination is
often more concise than a fully ﬂuent sentence and   ROUGE is a recall-based measure for summa-
compensates for the fact that the topic and the de-  rizations. This automatic metric counts the num-
scription of what happened to it do not appear in    ber of n-grams in the reference summaries that
the same sentence in the original story.             occur in the candidate and divides by the num-
                                                     ber of n-grams in the reference summaries. The
   Sentences (11) and (12) are the output of Hedge   size of the n-grams used by ROUGE is conﬁg-
Trimmer and Topiary for the same story for which     urable. ROUGE-n uses 1-grams through n-grams.
the topics in Sentence (10) were generated.          ROUGE-L is based on longest common subse-
                                                     quences, and ROUGE-W-1.2 is based on weighted
(11) FBI agents this week began questioning rel-     longest common subsequences with a weighting
      atives of the victims                          of 1.2 on consecutive matches of length greater
                                                     than 1.
(12) BIN LADEN EMBASSY BOMBING FBI
      agents this week began questioning relatives      The ROUGE scores for the nine systems and the
                                                     baseline are shown in Table 2. Under ROUGE-
   Topiary was submitted to the Document Under-      1 the Topiary variants scored signiﬁcantly higher
standing Conference Workshop. Figure 1 shows         than the Trimmer variants, and both scored signif-
how Topiary peformed in comparison with other
DUC2004 participants on task 1, using ROUGE.
Task 1 was to produce a summary for a single news
           0.35

           0.3

           0.25

           0.2

           0.15

           0.1

           0.05

           0                   ROUGE-L     ROUGE-W-1.2  ROUGE-2  ROUGE-3     ROUGE-4
                      ROUGE-1

                               Automatic Summaries      Reference Summaries  Topiary

Figure 1: ROUGE Scores for DUC2004 Automatic Summaries, Reference Summaries and Topiary

System     Description              Words  Chars        icantly higher than the UTD topic lists with 95%
Trim                                8.7    57.3         conﬁdence. Since ﬂuency is not measured at all
Trim.E     Trimmer                  8.7    57.1         by unigrams, we must conclude that the Trimmer
Trim.HB    no have/be removal       8.6    57.7         headlines, by selecting the lead sentence, included
Trim.HB.E  no emergency shortening  8.6    57.4         more or better topic words than UTD. The high-
Top        Trimmer                  10.8   73.3         est scoring UTD topics tend to be very meaning-
Top.E      no have/be removal       10.8   73.2         ful while the ﬁfth and lower scoring topics tend
Top.HB     emergency shortening     10.7   73.2         to be very noisy. Thus the higher scores of Topi-
Top.HB.E   Trimmer                  10.7   73.2         ary can be attributed to including only the best of
UTD        have/be removal          9.5    71.1         the UTD topics while preserving the lead sentence
           no emergency shortening                      topics. The same groupings occur with ROUGE-L
           Trimmer                                      and ROUGE-W, indicating that the longest com-
           have/be removal                              mon subsequences are dominated by sequences of
           emergency shortening                         length one.
           Topiary
           no have/be removal                              Under the higher order ROUGE evaluations
           no emergency shortening                      the systems group by the presence or absence of
           Topiary                                      have/be removal, with higher scores going to sys-
           no have/be removal                           tems in which have/be removal was performed.
           emergency shortening                         This indicates that the removal of these light con-
           Topiary                                      tent verbs makes the summaries more like the lan-
           have/be removal                              guage of headlines. The value of emergency short-
           no emergency shortening                      ening over truncation is not clear.
           Topiary
           have/be removal
           emergency shortening
           UTD Topics

Table 1: Systems and Headline Lengths
Top.HB.E   ROUGE-1  ROUGE-2  ROUGE-3                   ROUGE-4    ROUGE-L  ROUGE-W-1.2
Top.HB     0.24914  0.06449  0.02122                   0.00712    0.19951  0.11891
Top.E      0.24873  0.06595  0.02267                   0.00826    0.20061  0.11970
Top        0.24812  0.06169  0.01874                   0.00562    0.19856  0.11837
baseline   0.24621  0.06309  0.01995                   0.00639    0.19856  0.11861
Trim.HB.E  0.22136  0.06370  0.02118                   0.00707    0.11738  0.16955
Trim.HB    0.20415  0.06571  0.02527                   0.00950    0.18506  0.11127
Trim.E     0.20380  0.06565  0.02508                   0.00945    0.18472  0.11118
Trim       0.20105  0.06226  0.02221                   0.00774    0.18287  0.11003
UTD        0.20061  0.06283  0.02266                   0.00792    0.18248  0.10996
           0.15913  0.01585  0.00087                   0.00000    0.13041  0.07797

           Table 2: ROUGE Scores sorted by ROUGE-1

4.2 BLEU                                               Top.HB.E   BLEU-1  BLEU-2  BLEU-3  BLEU-4
                                                       Top.HB     0.4368  0.2443  0.1443  0.0849
BLEU is a system for automatic evaluation of ma-       Top.E      0.4362  0.2463  0.1476  0.0885
chine translation that uses a modiﬁed n-gram pre-      Top        0.4310  0.2389  0.1381  0.0739
cision measure to compare machine translations to      Trim.HB.E  0.4288  0.2415  0.1417  0.0832
reference human translations. This automatic met-      Trim.HB    0.3712  0.2333  0.1495  0.0939
ric counts the number of n-grams in the candidate      baseline   0.3705  0.2331  0.1493  0.0943
that occur in any of the reference summaries and       Trim.E     0.3695  0.2214  0.1372  0.0853
divides by the number of n-grams in the candidate.     Trim       0.3636  0.2285  0.1442  0.0897
The size of the n-grams used by BLEU is conﬁg-         UTD        0.3635  0.2297  0.1461  0.0922
urable. BLEU-n uses 1-grams through n-grams. In                   0.2859  0.0954  0.0263  0.0000
our evaluation of headline generation systems, we
treat summarization as a type of translation from      Table 3: BLEU Scores sorted by BLEU-1
a verbose language to a concise one, and compare
automatically generated headlines to human gen-        jects cannot perform with a high level of agree-
erated headlines.                                      ment – even when they are shown the entire docu-
                                                       ment – it will not be possible to detect signiﬁcant
   The BLEU scores for the nine systems and            differences among different summarization meth-
the baseline are shown in Table 3. For BLEU-1          ods because the amount of variation due to noise
the Topiary variants score signiﬁcantly better than    will overshadow the variation due to summariza-
the Trimmer variants with 95% conﬁdence. Un-           tion method.
der BLEU-2 the Topiary scores are higher than
the Trimmer scores, but not signiﬁcantly. Under           In an earlier experiment we attempted to use
BLEU-4 the Trimmer variants score slightly but         document selection in the context of informa-
not signiﬁcantly higher than the Topiary variants,     tion retrieval as an extrinsic task. Subjects were
and at BLEU-3 there is no clear pattern. Trim-         asked to decide if a document was highly rele-
mer and Topiary variants score signiﬁcantly higher     vant, somewhat relevant or not relevant to a given
than UTD for all settings of BLEU with 95% con-        query. However we found that subjects who had
ﬁdence.                                                been shown the entire document were only able
                                                       to agree with each other 75% of the time and
5 Extrinsic Task                                       agreed with the allegedly correct answers only
                                                       70% of the time. We were unable to draw any
For an automatic summarization evaluation tool to      conclusions about the relative performance of the
be of use to developers it must be shown to cor-       summarization systems, and thus were not able
relate well with human performance on a speciﬁc        to make any correlations between human perfor-
extrinsic task. In selecting the extrinsic task it is  mance and scores on automatic summarization
important that the task be unambiguous enough          evaluation tools. For more details see (Zajic et al.,
that subjects can perform it with a high level of      2004).
agreement. If the task is so difﬁcult that sub-
                                                          We propose a more constrained type of docu-
ment relevance judgment as an appropriate extrin-     Full Text  Precision  Recall  £¥¤§¦ ¨
sic task for evaluating human performance using       Headline   0.831      0.900
automatic summarizations. The task, event track-                 0.842      0.842   0.864
ing, has been reported in NIST TDT evaluations
to provide the basis for more reliable results. Sub-                                0.842
jects are asked to decide if a document contains
information related to a particular event in a spe-   Table 4: Results of Event Tracking Experiment
ciﬁc domain. The subject is told about a speciﬁc
event, such as the bombing of the Murrah Federal      with ©¥  .
Building in Oklahoma City. A detailed descrip-
tion is given about what information is considered       The small difference in NIST agreement be-
relevent to an event in the given domain. For in-     tween full texts and headlines seems to suggest
stance, in the criminal case domain, information      that the best human-written headlines can supply
about the crime, the investigation, the arrest, the   sufﬁcient information for performing event track-
trial and the sentence are relevant.                  ing. However it is possible that subjects found the
                                                      task of reading entire texts dull, and allowed their
   We performed a small event tracking experi-        performance to diminish as they grew tired.
ment to compare human performance using full
news story text against performance using human-         Full texts yielded a higher recall than head-
generated headlines of the same stories. Seven        lines, which is not surprising. However headlines
events and twenty documents per event were cho-       yielded a slightly higher precision than full texts
sen from the 1999 Topic Detection and Tracking        which means that subjects were able to reject non-
(TDT3) corpus. Four subjects were asked to judge      relevant documents as well with headlines as they
the full news story texts or story headlines as rel-  could by reading the entire document. We ob-
evant or not relevant to each speciﬁed event. The     served that subjects sometimes marked documents
documents in the TDT3 corpus were already an-         as relevant if the full text contained even a brief
notated as relevant or not relevant to each event     mention of the event or any detail that could be
by NIST annotators. The NIST annotations were         construed as satisfying the domain description. If
taken to be the correct answers by which to judge     avoiding false positives (or increasing precision) is
the overall performance of the subjects. The sub-     an important goal, these results suggest that use of
jects were shown a practice event, three events       headlines provides an advantage over reading the
with full story text and three events with story      entire text.
headlines.
                                                         Further event tracking experiments will include
   We calculated average agreement between sub-       a variety of methods for automatic summariza-
jects as the number of documents on which two         tion. This will give us the ability to compare hu-
subjects made the same judgment divided by the        man performance using the summarization meth-
number of documents on which the two subjects         ods against one another and against human perfor-
had both made judgments. The average agreement        mance using full text. We do not expect that any
between subjects was 86% for full story texts and     summarization method will allow humans to per-
80% for headlines. The average agreement with         form event tracking better than reading the entire
the NIST annotations was slightly higher when us-     document, however we hope that we can improve
ing the full story text than the headline, with text  human performance time while introducing only
producing 86% overall agreement with NIST and         a small, acceptable loss in performance. We also
headlines producing 84% agreement with NIST.          plan to calibrate automatic summarization evalu-
Use of headlines resulted in a signiﬁcant increase    ation tools, such as BLEU and ROUGE, to ac-
in speed. Subjects spent an average of 30 seconds     tual human performance on event tracking for each
per document when shown the entire text, but only     method.
7.7 seconds per document when shown the head-
                                                      6 Conclusions and Future Work
line. Table 4 shows the precision, recall and ¢¡
                                                      We have shown the effectiveness of combining
                                                      sentence compression and topic lists to construct
                                                      informative summaries. We have compared three
approaches to automatic headline generation (Top-          the American Association for Artiﬁcial Intelligence
iary, Hedge Trimmer and Unsupervised Topic Dis-            AAAI2000, Austin, Texas.
covery) using two automatic summarization evalu-
ation tools (BLEU and ROUGE). We have stressed          David Lewis. 1992. An evaluation of phrasal and clus-
the importance of correlating automatic evalua-            tered representations on a text categorization task.
tions with human performance of an extrinsic task,         In Proceedings of the 15th annual international
and have proposed event tracking as an appropri-           ACM SIGIR conference on Research and develop-
ate task for this purpose.                                 ment in information retrieval, pages 37–50, Copen-
                                                           hagen, Denmark.
   We plan to perform a study in which Topiary,
Hedge Trimmer, Unsupervised Topic Discovery             Chin-Yew Lin and Eduard Hovy. 2003. Auto-
and other summarization methods will be evalu-             matic Evaluation of Summaries Using N-gram Co-
ated in the context of event tracking. We also plan        Occurrences Statistics. In Proceedings of the Con-
to extend the tools described in this paper to the         ference of the North American Chapter of the As-
domains of transcribed broadcast news and cross-           sociation for Computational Linguistics, Edmonton,
language headline generation.                              Alberta.

Acknowledgements                                        Ingrid Ma˚rdh. 1980. Headlinese: On the Grammar of
                                                           English Front Page Headlines. Malmo.
The University of Maryland authors are sup-
ported, in part, by BBNT Contract 020124-               S. Miller, M. Crystal, H. Fox, L. Ramshaw,
7157, DARPA/ITO Contract N66001-97-C-8540,                 R. Schwartz, R. Stone, and R. Weischedel. 1998.
and NSF CISE Research Infrastructure Award                 Algorithms that Learn to Extract Information; BBN:
EIA0130422.                                                Description of the SIFT System as Used for MUC-7.
                                                           In Proceedings of the MUC-7.
References
                                                        K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Sabine Bergler, Rene´ Witte, Michelle Khalife,             Bleu: a Method for Automatic Evaluation of Ma-
   Zhuoyan Li, and Frank Rudzicz. 2003. Using              chine Translation. In Proceedings of Association of
   knowledge-poor coreference resolution for text sum-     Computational Linguistics, Philadelphia, PA.
   marization. In Proceedings of the 2003 Document
   Understanding Conference, Draft Papers, pages 85–    R. Schwartz, T. Imai, F. Jubala, L. Nguyen, and
   92, Edmonton, Candada.                                  J. Makhoul. 1997. A maximum likelihood model
                                                           for topic classiﬁcation of broadcast news. In
Bonnie Dorr, David Zajic, and Richard Schwartz.            Eurospeech-97, Rhodes, Greece.
   2003a. Cross-language headline generation for
   hindi. ACM Transactions on Asian Language Infor-     David Zajic, Bonnie Dorr, Richard Schwartz, and Stacy
   mation Processing (TALIP), 2:2.                         President. 2004. Headline evaluation experiment
                                                           results, umiacs-tr-2004-18. Technical report, Uni-
Bonnie Dorr, David Zajic, and Richard Schwartz.            versity of Maryland Institute for Advanced Comput-
   2003b. Hedge trimmer: A parse-and-trim approach         ing Studies, College Park, Maryland.
   to headline generation. In Proceedings of the HLT-
   NAACL 2003 Text Summarization Workshop, Ed-          Liang Zhou and Eduard Hovy. 2003. Headline sum-
   monton, Alberta, Canada, pages 1–8.                     marization at isi. In Proceedings of the 2003 Doc-
                                                           ument Understanding Conference, Draft Papers,
T. Euler. 2002. Tailoring text using topic words: Se-      pages 174–178, Edmonton, Candada.
   lection and compression. In Proceedings of 13th
   International Workshop on Database and Expert
   Systems Applications (DEXA 2002), 2-6 Septem-
   ber 2002, Aix-en-Provence, France, pages 215–222.
   IEEE Computer Society.

Kevin Knight and Daniel Marcu. 2000. Statistics-
   based summarization – step one: Sentence com-
   pression. In The 17th National Conference of
