                 Summary Evaluation
            with and without References

Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Vela´zquez-Morales

   Abstract—We study a new content-based method for                                automatically generated summary (peer) has to be compared
the evaluation of text summarization systems without                               with one or more reference summaries (models). DUC used
human models which is used to produce system rankings.                             an interface called SEE to allow human judges to compare
The research is carried out using a new content-based                              a peer with a model. Thus, judges give a COVERAGE score
evaluation framework called FRESA to compute a variety of                          to each peer produced by a system and the ﬁnal system
divergences among probability distributions. We apply our                          COVERAGE score is the average of the COVERAGE’s scores
comparison framework to various well-established content-based                     asigned. These system’s COVERAGE scores can then be used
evaluation measures in text summarization such as COVERAGE,                        to rank summarization systems. In the case of query-focused
RESPONSIVENESS, PYRAMIDS and ROUGE studying their                                  summarization (e.g. when the summary should answer a
associations in various text summarization tasks including                         question or series of questions) a RESPONSIVENESS score
generic multi-document summarization in English and French,                        is also assigned to each summary, which indicates how
focus-based multi-document summarization in English and                            responsive the summary is to the question(s).
generic single-document summarization in French and Spanish.
                                                                                      Because manual comparison of peer summaries with model
   Index Terms—Text summarization evaluation, content-based                        summaries is an arduous and costly process, a body of
evaluation measures, divergences.                                                  research has been produced in the last decade on automatic
                                                                                   content-based evaluation procedures. Early studies used text
                          I. INTRODUCTION                                          similarity measures such as cosine similarity (with or without
                                                                                   weighting schema) to compare peer and model summaries
T EXT summarization evaluation has always been a                                   [5]. Various vocabulary overlap measures such as n-grams
      complex and controversial issue in computational                             overlap or longest common subsequence between peer and
linguistics. In the last decade, signiﬁcant advances have been                     model have also been proposed [6], [7]. The BLEU machine
made in this ﬁeld as well as various evaluation measures have                      translation evaluation measure [8] has also been tested in
been designed. Two evaluation campaigns have been led by                           summarization [9]. The DUC conferences adopted the ROUGE
the U.S. agence DARPA. The ﬁrst one, SUMMAC, ran from                              package for content-based evaluation [10]. ROUGE implements
1996 to 1998 under the auspices of the Tipster program [1],                        a series of recall measures based on n-gram co-occurrence
and the second one, entitled DUC (Document Understanding                           between a peer summary and a set of model summaries. These
Conference) [2], was the main evaluation forum from 2000                           measures are used to produce systems’ rank. It has been shown
until 2007. Nowadays, the Text Analysis Conference (TAC)                           that system rankings, produced by some ROUGE measures
[3] provides a forum for assessment of different information                       (e.g., ROUGE-2, which uses 2-grams), have a correlation with
access technologies including text summarization.                                  rankings produced using COVERAGE.

   Evaluation in text summarization can be extrinsic or                               In recent years the PYRAMIDS evaluation method [11] has
intrinsic [4]. In an extrinsic evaluation, the summaries are                       been introduced. It is based on the distribution of “content”
assessed in the context of an speciﬁc task carried out by a                        of a set of model summaries. Summary Content Units (SCUs)
human or a machine. In an intrinsic evaluation, the summaries                      are ﬁrst identiﬁed in the model summaries, then each SCU
are evaluated in reference to some ideal model. SUMMAC                             receives a weight which is the number of models containing
was mainly extrinsic while DUC and TAC followed an                                 or expressing the same unit. Peer SCUs are identiﬁed in the
intrinsic evaluation paradigm. In an intrinsic evaluation, an                      peer, matched against model SCUs, and weighted accordingly.
                                                                                   The PYRAMIDS score given to a peer is the ratio of the sum
   Manuscript received June 8, 2010. Manuscript accepted for publication July      of the weights of its units and the sum of the weights of the
25, 2010.                                                                          best possible ideal summary with the same number of SCUs as
                                                                                   the peer. The PYRAMIDS scores can be also used for ranking
   Juan-Manuel Torres-Moreno is with LIA/Universite´ d’Avignon,                    summarization systems. [11] showed that PYRAMIDS scores
France and E´ cole Polytechnique de Montre´al, Canada                              produced reliable system rankings when multiple (4 or more)
(juan-manuel.torres@univ-avignon.fr).                                              models were used and that PYRAMIDS rankings correlate with
                                                                                   rankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGE
   Eric SanJuan is with LIA/Universite´ d’Avignon, France                          with skip 2-grams). However, this method requires the creation
(eric.sanjuan@univ-avignon.fr).

   Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain
(horacio.saggion@upf.edu).

   Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain;
LIA/Universite´ d’Avignon, France and Instituto de Ingenier´ıa/UNAM, Mexico
(iria.dacunha@upf.edu).

   Patricia Vela´zquez-Morales is with VM Labs, France
(patricia velazquez@yahoo.com).

                                                                               13  Polibits (42) 2010
Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

of models and the identiﬁcation, matching, and weighting of          non-random systems, no clear conclusion was reached on the
SCUs in both: models and peers.                                      value of each of the studied measures.

   [12] evaluated the effectiveness of the Jensen-Shannon               Nowadays, a widespread summarization evaluation
(J S) [13] theoretic measure in predicting systems ranks             framework is ROUGE [14], which offers a set of statistics
in two summarization tasks: query-focused and update                 that compare peer summaries with models. It counts
summarization. They have shown that ranks produced                   co-occurrences of n-grams in peer and models to derive a
by PYRAMIDS and those produced by J S measure                        score. There are several statistics depending on the used
correlate. However, they did not investigate the effect              n-grams and the text processing applied to the input texts
of the measure in summarization tasks such as generic                (e.g., lemmatization, stop-word removal).
multi-document summarization (DUC 2004 Task 2),
biographical summarization (DUC 2004 Task 5), opinion                   [15] proposed a method of evaluation based on the
summarization (TAC 2008 OS), and summarization in                    use of “distances” or divergences between two probability
languages other than English.                                        distributions (the distribution of units in the automatic
                                                                     summary and the distribution of units in the model
   In this paper we present a series of experiments aimed at         summary). They studied two different Information Theoretic
a better understanding of the value of the J S divergence            measures of divergence: the Kullback-Leibler (KL) [16] and
for ranking summarization systems. We have carried out               Jensen-Shannon (J S) [13] divergences. KL computes the
experimentation with the proposed measure and we have                divergence between probability distributions P and Q in the
veriﬁed that in certain tasks (such as those studied by              following way:
[12]) there is a strong correlation among PYRAMIDS,
RESPONSIVENESS and the J S divergence, but as we will                                   1                             Pw  log2  Pw      (1)
show in this paper, there are datasets in which the correlation      DKL(P ||Q) = 2                                             Qw
is not so strong. We also present experiments in Spanish                                                           w
and French showing positive correlation between the J S
and ROUGE which is the de facto evaluation measure used              While J S divergence is deﬁned as follows:
in evaluation of non-English summarization. To the best of
our knowledge this is the more extensive set of experiments                             1                Pw  log2     2Pw   + Qw  log2     2Qw
interpreting the value of evaluation without human models.           DJ S (P ||Q) = 2                              Pw + Qw              Pw + Qw
                                                                                           w
   The rest of the paper is organized in the following way:                                                                             (2)
First in Section II we introduce related work in the area of
content-based evaluation identifying the departing point for         These measures can be applied to the distribution of units in
our inquiry; then in Section III we explain the methodology
adopted in our work and the tools and resources used for             system summaries P and reference summaries Q. The value
experimentation. In Section IV we present the experiments
carried out together with the results. Section V discusses the       obtained may be used as a score for the system summary. The
results and Section VI concludes the paper and identiﬁes future
work.                                                                method has been tested by [15] over the DUC 2002 corpus for

                         II. RELATED WORK                            single and multi-document summarization tasks showing good

   One of the ﬁrst works to use content-based measures in            correlation among divergence measures and both coverage and
text summarization evaluation is due to [5], who presented an
evaluation framework to compare rankings of summarization            ROUGE rankings.
systems produced by recall and cosine-based measures. They
showed that there was weak correlation among rankings                [12] went even further and, as in [5], they proposed to
produced by recall, but that content-based measures produce
rankings which were strongly correlated. This put forward            compare directly the distribution of words in full documents
the idea of using directly the full document for comparison
purposes in text summarization evaluation. [6] presented a           with the distribution of words in automatic summaries to
set of evaluation measures based on the notion of vocabulary
overlap including n-gram overlap, cosine similarity, and             derive a content-based evaluation measure. They found a
longest common subsequence, and they applied them to
multi-document summarization in English and Chinese.                 high correlation between rankings produced using models
However, they did not evaluate the performance of the
measures in different summarization tasks. [7] also compared         and rankings produced without models. This last work is the
various evaluation measures based on vocabulary overlap.
Although these measures were able to separate random from            departing point for our inquiry into the value of measures that

                                                                     do not rely on human models.

                                                                                             III. METHODOLOGY

                                                                        The followed methodology in this paper mirrors the one
                                                                     adopted in past work (e.g. [5], [7], [12]). Given a particular
                                                                     summarization task T , p data points to be summarized
                                                                     with input material {Ii}ip=−01 (e.g. document(s), question(s),
                                                                     topic(s)), s peer summaries {SUMi,k}ks−=10 for input i, and
                                                                     m model summaries {MODELi,j}jm=−01 for input i, we will
                                                                     compare rankings of the s peer summaries produced by various
                                                                     evaluation measures. Some measures that we use compare
                                                                     summaries with n of the m models:

                                                                     MEASUREM (SUMi,k, {MODELi,j }jn=−01)                               (3)

Polibits (42) 2010                                               14
                                                                               Summary Evaluation with and without References

while other measures compare peers with all or some of the                        3) Update-summarization task that consists of creating a
input material:                                                                       summary out of a cluster of documents and a topic. Two
                                                                                      sub-tasks are considered here: A) an initial summary has
MEASUREM (SUMi,k, Ii)  (4)                                                            to be produced based on an initial set of documents and
                                                                                      topic; B) an update summary has to be produced from
where Ii is some subset of input Ii. The values produced                              a different (but related) cluster assuming documents
by the measures for each summary SUMi,k are averaged                                  used in A) are known. The English TAC’08 Update
for each system k = 0, . . . , s − 1 and these averages are                           Summarization dataset is used, which consists of 48
used to produce a ranking. Rankings are then compared                                 topics with 20 documents each – 36,911 words.
using Spearman Rank correlation [17] which is used to
measure the degree of association between two variables                           4) Opinion summarization where systems have to analyze
whose values are used to rank objects. We have chosen                                 a set of blog articles and summarize the opinions
to use this correlation to compare directly results to those                          about a target in the articles. The TAC’08 Opinion
presented in [12]. Computation of correlations is done using                          Summarization in English4 data set (taken from the
the Statistics-RankCorrelation-0.12 package1, which computes                          Blogs06 Text Collection) is used: 25 clusters and targets
the rank correlation between two vectors. We also veriﬁed                             (i.e., target entity and questions) were used – 1,167,735
the good conformity of the results with the correlation test                          words.
of Kendall τ calculated with the statistical software R. The
two nonparametric tests of Spearman and Kendall do not                            5) Generic single-document summarization in Spanish
really stand out as the treatment of ex-æquo. The good                                using the Medicina Cl´ınica5 corpus, which is composed
correspondence between the two tests shows that they do not                           of 50 medical articles in Spanish, each one with its
introduce bias in our analysis. Subsequently will mention only                        corresponding author abstract – 124,929 words.
the ρ of Sperman more widely used in this ﬁeld.
                                                                                  6) Generic single document summarization in French using
A. Tools                                                                              the “Canadien French Sociological Articles” corpus
                                                                                      from the journal Perspectives interdisciplinaires sur le
   We carry out experimentation using a new summarization                             travail et la sante´ (PISTES)6. It contains 50 sociological
evaluation framework: FRESA –FRamework for Evaluating                                 articles in French, each one with its corresponding
Summaries Automatically–, which includes document-based                               author abstract – 381,039 words.
summary evaluation measures based on probabilities
distribution2. As in the ROUGE package, FRESA supports                            7) Generic multi-document-summarization in French using
different n-grams and skip n-grams probability distributions.                         data from the RPM27 corpus [18], 20 different themes
The FRESA environment can be used in the evaluation of                                consisting of 10 articles and 4 abstracts by reference
summaries in English, French, Spanish and Catalan, and it                             thematic – 185,223 words.
integrates ﬁltering and lemmatization in the treatment of
summaries and documents. It is developed in Perl and will                      For experimentation in the TAC and the DUC datasets we use
be made publicly available. We also use the ROUGE package                      directly the peer summaries produced by systems participating
[10] to compute various ROUGE statistics in new datasets.                      in the evaluations. For experimentation in Spanish and French
                                                                               (single and multi-document summarization) we have created
B. Summarization Tasks and Data Sets                                           summaries at a similar ratio to those of reference using the
                                                                               following systems:
   We have conducted our experimentation with the following
summarization tasks and data sets:                                                – ENERTEX [19], a summarizer based on a theory of
                                                                                     textual energy;
   1) Generic multi-document-summarization in English
       (production of a short summary of a cluster of related                     – CORTEX [20], a single-document sentence extraction
       documents) using data from DUC’043, task 2: 50                                system for Spanish and French that combines various
       clusters, 10 documents each – 294,636 words.                                  statistical measures of relevance (angle between sentence
                                                                                     and topic, various Hamming weights for sentences, etc.)
   2) Focused-based summarization in English (production of                          and applies an optimal decision algorithm for sentence
       a short focused multi-document summary focused on the                         selection;
       question “who is X?”, where X is a person’s name) using
       data from the DUC’04 task 5: 50 clusters, 10 documents                     – SUMMTERM [21], a terminology-based summarizer that
       each plus a target person name – 284,440 words.                               is used for summarization of medical articles and
                                                                                     uses specialized terminology for scoring and ranking
                                                                                     sentences;

                                                                                  – REG [22], summarization system based on an greedy
                                                                                     algorithm;

   1http://search.cpan.org/∼gene/Statistics-RankCorrelation-0.12/              4http://www.nist.gov/tac/data/index.html
   2FRESA is available at: http://lia.univavignon.fr/ﬁleadmin/axes/TALNE/      5http://www.elsevier.es/revistas/ctl servlet? f=7032&revistaid=2
                                                                               6http://www.pistes.uqam.ca/
Ressources.html                                                                7http://www-labs.sinequa.com/rpm2
   3http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html

                                                                           15  Polibits (42) 2010
Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

   – J S summarizer, a summarization system that scores            presented here we used uni-grams, 2-grams, and the skip
      and ranks sentences according to their Jensen-Shannon
      divergence to the source document;                           2-grams with maximum skip distance of 4 (ROUGE-1,

   – a lead-based summarization system that selects the lead       ROUGE-2 and ROUGE-SU4). ROUGE is used to compare
      sentences of the document;
                                                                   a peer summary to a set of model summaries in our
   – a random-based summarization system that selects
      sentences at random;                                         framework (as indicated in equation 3).

   – Open Text Summarizer [23], a multi-lingual summarizer         – Jensen-Shannon divergence formula given in Equation 2
      based on the frequency and
                                                                   is implemented in our FRESA package with the following
   – commercial systems: Word, SSSummarizer8, Pertinence9
      and Copernic10.                                              speciﬁcation (Equation 6) for the probability distribution

C. Evaluation Measures                                             of words w.

   The following measures derived from human assessment of                                                           Pw  =  CwT
the content of the summaries are used in our experiments:                                                                   N

   – COVERAGE is understood as the degree to which one                          Qw =                            CwS  if w ∈ S    (6)
      peer summary conveys the same information as a model                                                      NS   otherwise
      summary [2]. COVERAGE was used in DUC evaluations.                                                  CwT +δ
      This measure is used as indicated in equation 3 using                                              N +δ∗B
      human references or models.
                                                                   Where P is the probability distribution of words w in
   – RESPONSIVENESS ranks summaries in a 5-point scale
      indicating how well the summary satisﬁed a given             text T and Q is the probability distribution of words w
      information need [2]. It is used in focused-based
      summarization tasks. This measure is used as indicated       in summary S; N is the number of words in text and
      in equation 4 since a human judges the summary               summary N = NT + NS, B = 1.5|V |, CwT is the number
      with respect to a given input “user need” (e.g., a           of words in the text and CwS is the number of words in
      question). RESPONSIVENESS was used in DUC and TAC            the summary. For smoothing the summary’s probabilities
      evaluations.
                                                                   we have used δ = 0.005. We have also implemented
   – PYRAMIDS [11] is a content assessment measure which
      compares content units in a peer summary to weighted         other smoothing approaches (e.g. Good-Turing [24], that
      content units in a set of model summaries. This
      measure is used as indicated in equation 3 using human       uses the CPAN Perl’s Statistics-Smoothing-SGT-2.1.2
      references or models. PYRAMIDS is the adopted metric         package11) in FRESA, but we do not use them in
      for content-based evaluation in the TAC evaluations.
                                                                   the experiments reported here. Following the ROUGE
For DUC and TAC datasets the values of these measures are
available and we used them directly. We used the following         approach, in addition to word uni-grams we use 2-grams
automatic evaluation measures in our experiments:
                                                                   and skip n-grams computing divergences such as J S
   – ROUGE [14], which is a recall metric that takes into
      account n-grams as units of content for comparing peer       (using uni-grams) J S2 (using 2-grams), J S4 (using the
      and model summaries. The ROUGE formula speciﬁed in           skip n-grams of ROUGE-SU4), and J SM which is an
      [10] is as follows:                                          average of the J Si. J Ss measures are used to compare a
                                                                   peer summary to its source document(s) in our framework
                                         ROUGE-n(R, M ) =
            m ∈ M n−gram∈P countmatch(n − gram) (5)                (as indicated in equation 4). In the case of summarization

                        m ∈ M count(n-gram)                        of multiple documents, these are concatenated (in the

      where R is the summary to be evaluated, M is the set of      given input order) to form a single input from which
      model (human) summaries, countmatch is the number of
      common n-grams in m and P , and count is the number          probabilities are computed.
      of n-grams in the model summaries. For the experiments
                                                                                   IV. EXPERIMENTS AND RESULTS
   8http://www.kryltech.com/summarizer.htm
   9http://www.pertinence.net                                         We ﬁrst replicated the experiments presented in [12] to
   10http://www.copernic.com/en/products/summarizer                verify that our implementation of J S produced correlation
                                                                   results compatible with that work. We used the TAC’08
                                                                   Update Summarization data set and computed J S and
                                                                   ROUGE measures for each peer summary. We produced
                                                                   two system rankings (one for each measure), which were
                                                                   compared to rankings produced using the manual PYRAMIDS
                                                                   and RESPONSIVENESS scores. Spearman correlations were
                                                                   computed among the different rankings. The results are
                                                                   presented in Table I. These results conﬁrm a high correlation
                                                                   among PYRAMIDS, RESPONSIVENESS and J S. We also
                                                                   veriﬁed high correlation between J S and ROUGE-2 (0.83
                                                                   Spearman correlation, not shown in the table) in this task and
                                                                   dataset.

                                                                      Then, we experimented with data from DUC’04, TAC’08
                                                                   Opinion Summarization pilot task as well as single and

                                                                      11http://search.cpan.org/∼bjoernw/Statistics-Smoothing-SGT-2.1.2/

Polibits (42) 2010                                             16
                                                                     Summary Evaluation with and without References

                                       TABLE I                       evaluation metrics that do not rely on human models but that
SPEARMAN CORRELATION OF CONTENT-BASED MEASURES IN TAC’08             compare summary content to input content directly [12]. We
                                                                     have some positive and some negative results regarding the
                        UPDATE SUMMARIZATION TASK                    direct use of the full document in content-based evaluation.

Mesure   PYRAMIDS    p-value  RESPONSIVENESS    p-value                 We have veriﬁed that in both generic muti-document
ROUGE-2      0.96  p < 0.005          0.92    p < 0.005              summarization and in topic-based multi-document
             0.85  p < 0.005          0.74    p < 0.005              summarization in English correlation among measures
   JS                                                                that use human models (PYRAMIDS, RESPONSIVENESS
                                                                     and ROUGE) and a measure that does not use models
multi-document summarization in Spanish and French. In spite         (J S divergence) is strong. We have found that correlation
of the fact that the experiments for French and Spanish corpora      among the same measures is weak for summarization of
use less data points (i.e., less summarizers per task) than          biographical information and summarization of opinions in
for English, results are still quite signiﬁcant. For DUC’04,         blogs. We believe that in these cases content-based measures
we computed the J S measure for each peer summary in                 should be considered, in addition to the input document, the
tasks 2 and 5 and we used J S, ROUGE, COVERAGE and                   summarization task (i.e. text-based representation, description)
RESPONSIVENESS scores to produce systems’ rankings. The              to better assess the content of the peers [25], the task being a
various Spearman’s rank correlation values for DUC’04 are            determinant factor in the selection of content for the summary.
presented in Tables II (for task 2) and III (for task 5).
For task 2, we have veriﬁed a strong correlation between                Our multi-lingual experiments in generic single-document
J S and COVERAGE. For task 5, the correlation between                summarization conﬁrm a strong correlation among the
J S and COVERAGE is weak, and that between J S and                   J S divergence and ROUGE measures. It is worth noting
RESPONSIVENESS is weak and negative.                                 that ROUGE is in general the chosen framework for
                                                                     presenting content-based evaluation results in non-English
   Although the Opinion Summarization (OS) task is a new             summarization.
type of summarization task and its evaluation is a complicated
issue, we have decided to compare J S rankings with those               For the experiments in Spanish, we are conscious that we
obtained using PYRAMIDS and RESPONSIVENESS in TAC’08.                only have one model summary to compare with the peers.
Spearman’s correlation values are listed in Table IV. As it can      Nevertheless, these models are the corresponding abstracts
be seen, there is weak and negative correlation of J S with          written by the authors. As the experiments in [26] show, the
both PYRAMIDS and RESPONSIVENESS. Correlation between                professionals of a specialized domain (as, for example, the
PYRAMIDS and RESPONSIVENESS rankings is high for this                medical domain) adopt similar strategies to summarize their
task (0.71 Spearman’s correlation value).                            texts and they tend to choose roughly the same content chunks
                                                                     for their summaries. Previous studies have shown that author
   For experimentation in mono-document summarization                abstracts are able to reformulate content with ﬁdelity [27] and
in Spanish and French, we have run 11 multi-lingual                  these abstracts are ideal candidates for comparison purposes.
summarization systems; for experimentation in French, we             Because of this, the summary of the author of a medical article
have run 12 systems. In both cases, we have produced                 can be taken as reference for summaries evaluation. It is worth
summaries at a compression rate close to the compression rate        noting that there is still debate on the number of models to be
of the authors’ provided abstracts. We have then computed J S        used in summarization evaluation [28]. In the French corpus
and ROUGE measures for each summary and we have averaged             PISTES, we suspect the situation is similar to the Spanish
the measure’s values for each system. These averages were            case.
used to produce rankings per each measure. We computed
Spearman’s correlations for all pairs of rankings.                               VI. CONCLUSIONS AND FUTURE WORK

   Results are presented in Tables V, VI and VII. All results           This paper has presented a series of experiments in
show medium to strong correlation between the J S measures           content-based measures that do not rely on the use of model
and ROUGE measures. However the J S measure based on                 summaries for comparison purposes. We have carried out
uni-grams has lower correlation than J Ss which use n-grams          extensive experimentation with different summarization tasks
of higher order. Note that table VII presents results for            drawing a clearer picture of tasks where the measures could
generic multi-document summarization in French, in this              be applied. This paper makes the following contributions:
case correlation scores are lower than correlation scores for
single-document summarization in French, a result which may             – We have shown that if we are only interested in ranking
be expected given the diversity of input in multi-document                 summarization systems according to the content of their
summarization.                                                             automatic summaries, there are tasks were models could
                                                                           be subtituted by the full document in the computation of
                           V. DISCUSSION                                   the J S measure obtaining reliable rankings. However,
                                                                           we have also found that the substitution of models
   The departing point for our inquiry into text summarization             by full-documents is not always advisable. We have
evaluation has been recent work on the use of content-based

                                                                 17  Polibits (42) 2010
Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales

                                                                   TABLE II
                    SPEARMAN ρ OF CONTENT-BASED MEASURES WITH COVERAGE IN DUC’04 TASK 2

                                     Mesure     COVERAGE                p-value
                                     ROUGE-2        0.79             p < 0.0050
                                                    0.68             p < 0.0025
                                        JS

                                                               TABLE III
                            SPEARMAN ρ OF CONTENT-BASED MEASURES IN DUC’04 TASK 5

                            Mesure   COVERAGE     p-value            RESPONSIVENESS   p-value
                            ROUGE-2      0.78   p < 0.001                    0.44    p < 0.05
                                         0.40   p < 0.050                    -0.18   p < 0.25
                               JS

                                                                TABLE IV
                            SPEARMAN ρ OF CONTENT-BASED MEASURES IN TAC’08 OS TASK

                            Mesure   PYRAMIDS p-value RESPONSIVENESS p-value
                              JS
                                     -0.13      p < 0.25             -0.14           p < 0.25

                                                                                TABLE V
                    SPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE Medicina Cl´ınica CORPUS (SPANISH)

                    Mesure  ROUGE-1    p-value  ROUGE-2                p-value   ROUGE-SU4                 p-value
                    JS         0.56  p < 0.100     0.46              p < 0.100        0.45               p < 0.200
                    J S2       0.88  p < 0.001     0.80              p < 0.002        0.81               p < 0.005
                    J S4       0.88  p < 0.001     0.80              p < 0.002        0.81               p < 0.005
                    J SM       0.82  p < 0.005     0.71              p < 0.020        0.71               p < 0.010

      found weak correlation among different rankings in             a representation of the task/topic in the calculation of
      complex summarization tasks such as the summarization          measures. To carry out these comparisons, however, we are
      of biographical information and the summarization of           dependent on the existence of references.
      opinions.
   – We have also carried out large-scale experiments in                FRESA will also be used in the new question-answer task
      Spanish and French which show positive medium to               campaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/
      strong correlation among system’s ranks produced by            qa.asp) for the evaluation of long answers. This task aims
      ROUGE and divergence measures that do not use the              to answer a question by extraction and agglomeration of
      model summaries.                                               sentences in Wikipedia. This kind of task corresponds
   – We have also presented a new framework, FRESA, for              to those for which we have found a high correlation
      the computation of measures based on J S divergence.           among the measures J S and evaluation methods with
      Following the ROUGE approach, FRESA package use                human intervention. Moreover, the J S calculation will be
      word uni-grams, 2-grams and skip n-grams computing             among the summaries produced and a representative set of
      divergences. This framework will be available to the           relevant passages from Wikipedia. FRESA will be used to
      community for research purposes.                               compare three types of systems, although different tasks: the
                                                                     multi-document summarizer guided by a query, the search
Although we have made a number of contributions, this paper          systems targeted information (focused IR) and the question
leaves many open questions than need to be addressed. In             answering systems.
order to verify correlation between ROUGE and J S, in the
short term we intend to extend our investigation to other                                    ACKNOWLEDGMENT
languages such as Portuguese and Chinesse for which we
have access to data and summarization technology. We also               We are grateful to the Programa Ramo´n y Cajal from
plan to apply FRESA to the rest of the DUC and TAC                   Ministerio de Ciencia e Innovacio´n, Spain. This work is
summarization tasks, by using several smoothing techniques.          partially supported by: a postdoctoral grant from the National
As a novel idea, we contemplate the possibility of adapting          Program for Mobility of Research Human Resources (National
the evaluation framework for the phrase compression task             Plan of Scientiﬁc Research, Development and Innovation
[29], which, to our knowledge, does not have an efﬁcient             2008-2011, Ministerio de Ciencia e Innovacio´n, Spain); the
evaluation measure. The main idea is to calculate J S from           research project CONACyT, number 82050, and the research
an automatically-compressed sentence taking the complete             project PAPIIT-DGAPA (Universidad Nacional Auto´noma de
sentence by reference. In the long term, we plan to incorporate      Me´xico), number IN403108.

Polibits (42) 2010                                               18
                                                                                                Summary Evaluation with and without References

                                                      TABLE VI
SPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE PISTES CORPUS (FRENCH)

Mesure   ROUGE-1    p-value  ROUGE-2                                                   p-value  ROUGE-SU4    p-value
JS          0.70  p < 0.050     0.73                                                  p < 0.05       0.73  p < 0.500
J S2        0.93  p < 0.002     0.86                                                  p < 0.01       0.86  p < 0.005
J S4        0.83  p < 0.020     0.76                                                  p < 0.05       0.76  p < 0.050
J SM        0.88  p < 0.010     0.83                                                  p < 0.02       0.83  p < 0.010

                                                    TABLE VII
SPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE RPM2 CORPUS (FRENCH)

Measure  ROUGE-1    p-value  ROUGE-2                                                   p-value  ROUGE-SU4   p-value
JS         0.830  p < 0.002    0.660                                                  p < 0.05      0.741  p < 0.01
J S2       0.800  p < 0.005    0.590                                                  p < 0.05      0.680  p < 0.02
J S4       0.750  p < 0.010    0.520                                                  p < 0.10      0.620  p < 0.05
J SM       0.850  p < 0.002    0.640                                                  p < 0.05      0.740  p < 0.01

                             REFERENCES                                               [18] C. de Loupy, M. Gue´gan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,
                                                                                            “A French Human Reference Corpus for multi-documents
 [1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and                              summarization and sentence compression,” in LREC’10, vol. 2,
      B. Sundheim, “Summac: a text summarization evaluation,” Natural                       Malta, 2010, p. In press.
      Language Engineering, vol. 8, no. 1, pp. 43–68, 2002.
                                                                                      [19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energy
 [2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,                       of Associative Memories: performants applications of Enertex algorithm
      no. 6, pp. 1506–1520, 2007.                                                           in text summarization and topic segmentation,” in MICAI’07, 2007, pp.
                                                                                            861–871.
 [3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,
      USA: NIST, November 17-19 2008.                                                 [20] J.-M. Torres-Moreno, P. Vela´zquez-Morales, and J.-G. Meunier,
                                                                                            “Condense´s de textes par des me´thodes nume´riques,” in JADT’02, vol. 2,
 [4] K. Spa¨rck Jones and J. Galliers, Evaluating Natural Language                          St Malo, France, 2002, pp. 723–734.
      Processing Systems, An Analysis and Review, ser. Lecture Notes in
      Computer Science. Springer, 1996, vol. 1083.                                    [21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Vela´zquez-Morales,
                                                                                            “Automatic summarization using terminological and semantic
 [5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison of                       resources,” in LREC’10, vol. 2, Malta, 2010, p. In press.
      rankings produced by summarization evaluation measures,” in NAACL
      Workshop on Automatic Summarization, 2000, pp. 69–78.                           [22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme glouton
                                                                                            applique´ au re´sume´ automatique de texte,” in JADT’10. Rome, 2010,
 [6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluation                          p. In press.
      of Summaries in a Cross-lingual Environment using Content-based
      Metrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.             [23] V. Yatsko and T. Vishnyakov, “A method for evaluating modern
                                                                                            systems of automatic text summarization,” Automatic Documentation
 [7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. C¸ elebi,            and Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007.
      D. Liu, and E. Dra´bek, “Evaluation challenges in large-scale document
      summarization,” in ACL’03, 2003, pp. 375–382.                                   [24] C. D. Manning and H. Schu¨tze, Foundations of Statistical Natural
                                                                                            Language Processing. Cambridge, Massachusetts: The MIT Press,
 [8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method                      1999.
      for automatic evaluation of machine translation,” in ACL’02, 2002, pp.
      311–318.                                                                        [25] K. Spa¨rck Jones, “Automatic summarising: The state of the art,” IPM,
                                                                                            vol. 43, no. 6, pp. 1449–1481, 2007.
 [9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation
      Initiatives in Natural Language Processing. Budapest, Hungary: EACL,            [26] I. da Cunha, L. Wanner, and M. T. Cabre´, “Summarization of specialized
      14 April 2003.                                                                        discourse: The case of medical articles in spanish,” Terminology, vol. 13,
                                                                                            no. 2, pp. 249–286, 2007.
[10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of
      Summaries,” in Text Summarization Branches Out: ACL-04 Workshop,                [27] C.-K. Chuah, “Types of lexical substitution in abstracting,” in ACL
      M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.                 Student Research Workshop. Toulouse, France: Association for
                                                                                            Computational Linguistics, 9-11 July 2001 2001, pp. 49–54.
[11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in
      Summarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.                     [28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries:
      145–152.                                                                              Metrics under varying data conditions,” in UCNLG+Sum’09, Suntec,
                                                                                            Singapore, August 2009, pp. 23–30.
[12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selection
      in Summarization without Human Models,” in Empirical Methods in                 [29] K. Knight and D. Marcu, “Statistics-based summarization-step one:
      Natural Language Processing, Singapore, August 2009, pp. 306–314.                     Sentence compression,” in Proceedings of the National Conference on
      [Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032                   Artiﬁcial Intelligence. Menlo Park, CA; Cambridge, MA; London;
                                                                                            AAAI Press; MIT Press; 1999, 2000, pp. 703–710.
[13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE
      Transactions on Information Theory, vol. 37, no. 145-151, 1991.

[14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using
      N-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,
      USA: Association for Computational Linguistics, 2003, pp. 71–78.

[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic
      approach to automatic evaluation of summaries,” in HLT-NAACL,
      Morristown, USA, 2006, pp. 463–470.

[16] S. Kullback and R. Leibler, “On information and sufﬁciency,” Ann. of
      Math. Stat., vol. 22, no. 1, pp. 79–86, 1951.

[17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral
      Sciences. McGraw-Hill, 1998.

                                                                                  19                                  Polibits (42) 2010
