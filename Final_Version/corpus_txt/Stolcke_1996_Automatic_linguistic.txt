AUTOMATIC LINGUISTIC SEGMENTATION
        OF CONVERSATIONAL SPEECH

Andreas Stolcke                                                          Elizabeth Shriberg

 Speech Technology and Research Laboratory
   SRI International, Menlo Park, CA 94025

stolcke@speech.sri.com ees@speech.sri.com

                      ABSTRACT                                           could use the knowledge incorporated in an automatic segmenter
                                                                         to help end-point a user’s speech input. A speech indexing and re-
As speech recognition moves toward more unconstrained domains            trieval system (such as for transcribed broadcast audio) could pro-
such as conversational speech, we encounter a need to be able to         cess its data in more meaningful units if the locations of linguistic
segment (or resegment) waveforms and recognizer output into lin-         segment boundaries were known.
guistically meaningful units, such a sentences. Toward this end,
we present a simple automatic segmenter of transcripts based on          Our main motivation for the work reported here comes from speech
N-gram language modeling. We also study the relevance of sev-            language modeling. Experiments at the 1995 Johns Hopkins Lan-
eral word-level features for segmentation performance. Using only        guage Modeling Workshop showed that the quality of a language
word-level information, we achieve 85% recall and 70% precision          model (LM) can be improved if both training and test data are seg-
on linguistic boundary detection.                                        mented linguistically, rather than acoustically [8]. We showed in
                                                                         [10] and [9] that proper modeling of ﬁlled pauses requires knowl-
               1. INTRODUCTION                                           edge of linguistic segment boundaries. We found for example that
                                                                         segment-internal ﬁlled pauses condition the following words quite
Today’s large-vocabulary speech recognizers typically prefer to pro-     differently from segment-initial ﬁlled pauses. Finally, recent efforts
cess a few tens of seconds of speech at a time, to keep the time and     in language modeling for conversational speech, such as [8], attempt
memory demands of the decoder within bounds. For longer inputs,          to capitalize on the internal structure of utterances and turns. Such
the waveform is usually presegmented into shorter pieces based on        models are formulated in terms of linguistic units and therefore re-
simple acoustic criteria, such as nonspeech intervals (e.g., pauses)     quire linguistic segmentations to be applicable.
and turn boundaries (when several speakers are involved). We refer
to such segmentations as acoustic segmentations.                                             3. METHOD

Acoustic segmentations generally do not reﬂect the linguistic struc-     Our main goal for this work was to examine to what extent various
ture of utterances. They may fragment sentences or semantic units,       kinds of lexical (word-based) information were useful for automatic
or group together spans of unrelated units. We examine several rea-      linguistic segmentation. This precluded a study based on the out-
sons why such behavior is undesirable, and propose that linguistic       put of existing speech recognition systems, which currently achieve
segmentations be used instead. This requires algorithms for auto-        about 40-50% word error rate on the type of data used in our exper-
matically ﬁnding linguistic units. In this paper we report on ﬁrst       iments. At such high error rates, the analysis of any segmentation
results from our ongoing efforts toward such an automatic linguis-       algorithm and the features it uses would likely be confounded by
tic segmentation. In all further discussion, unless otherwise noted,     the unreliable nature of the input data. We therefore chose to elimi-
the terms ‘segment,’ ‘segmentation,’ etc. will refer to linguistic seg-  nate the problem of inaccurate speech recognition and tested our al-
mentations.                                                              gorithms on hand-transcribed word-level transcripts of spontaneous
                                                                         speech from the Switchboard corpus [4]. An additional beneﬁt of
  2. THE IMPORTANCE OF LINGUISTIC                                        this approach is that the models employed by the segmentation al-
                    SEGMENTATION                                         gorithms can also be directly used as language models for speech
                                                                         recognizers for the same type of data, an application we are pursu-
Acoustic segmentations are inadequate in cases where the output          ing as well.
of a speech recognizer is to serve as input for further processing
based on syntactically or semantically coherent units. This includes     The segmentation approaches we investigated all fell within the fol-
most natural language (NL) parsers or NL understanding or transla-       lowing framework. We ﬁrst trained a statistical language model
tion systems. For such systems, the fragmented recognition output        of the N-gram variety to model the distribution of both words and
would have to be put back together and large spans of unrelated          segment boundaries. (For this purpose, segment boundaries were
material would need to be resegmented into linguistic units.             represented as special tokens <s> within the text.) The segmenta-
                                                                         tion information was removed from the test data, and the language
Automatic detection of linguistic segments could also improve the        model was used to hypothesize the most probable locations of seg-
user interface of many speech systems. A spoken language system
ment boundaries. The resulting segmentations were then evaluated          computation yields the likelihoods of the states at each position k:
along a number of metrics.
                                                                                PNO-S(w1 : : : wk) = PNO-S(w1 : : : wk 1)
As training data, we used 1.4 million words of Switchboard tran-
scripts annotated for linguistic segmentations by the UPenn Tree-                                  j p(wk wk 2wk 1)
bank project [7], comprising a total of 193,000 segments. One half
of the standard Switchboard development test set, totaling 10,000                                             +PS(w1 : : : wk 1)
words and 1,300 segments, was used for testing.
                                                                                                   j p(wk <s>wk 1)
The hand-annotated segments encompassed different kinds of lin-
guistic structures, including                                                        PS(w1 : : : wk) = PNO-S(w1 : : : wk 1)

      Complete sentences                                                                           j j p(<s> wk 2wk 1)p(wk <s>)

      Stand-alone phrases                                                                                     +PS(w1 : : : wk 1)
      Disﬂuent sentences aborted in mid-utterance1
                                                                                                   j j p(<s> <s>wk 1)p(wk <s>)
      Interjections and back-channel responses
                                                                          A corresponding Viterbi algorithm is used to ﬁnd the most likely
The following excerpt illustrates the character of the data. Linguis-     sequence of S and NO-S (i.e., a segmentation) for a given word
tic segment boundaries are marked <s>, whereas acoustic segmen-           string. This language model is a full implementation of the model
tations are indicated by //.                                              approximated in [8]. The hidden disﬂuency model of [10] has a
                                                                          similar structure. As indicated in the formulae above, we currently
       B.44: Worried that they're not going to                            use at most two words of history in the local conditional probabili-
       get enough attention? <s> //
       A.45: Yeah, <s> and, uh, you know, colds                           j ties p( ). Longer N-grams can be used if more state information is
       and things like that <laughter> get -- //
       B.46: Yeah. <s> //                                                 kept.
       A.47: -- spread real easy and things,
       <s> but, // and they're expensive <s> and,                         The local N-gram probabilities are estimated from the training data
       // <lipsmack> // course, // there's a lot                          by using Katz backoff with Good-Turing discounting [6].
       of different types of day care available,
       too, // you know, where they teach them                                                 5. RESULTS
       academic things. <s> //
       B.48: Yes. <s> //                                                  5.1. Baseline Segmentation Model

This short transcript shows some of the ubiquitous features of spon-      The ﬁrst model we looked at models only plain words and segment
taneous speech affecting segmentation, such as                            boundaries in the manner described. It was applied to the concate-
                                                                          nation of all turns of a conversation side, with no additional con-
                                                                          textual cues supplied. During testing, this model thus operates with
                                                                          very minimal information, i.e., with only the raw word sequence to
                                                                          be segmented. Table 1 shows results for bigram and trigram mod-
                                                                          els. The performance metrics used are deﬁned as follows. Recall

Mismatch between acoustic and linguistic segmentations                          Table 1: Baseline model performance
(A.47)                                                                    Model Recall Precision FA SER
                                                                          Bigram 65.5% 56.9% 1.9% 58.9%
segments spanning several turns (A.45 and A.47)                           Trigram 70.2% 60.7% 2.0% 53.1%

backchannel responses (B.46)

                  4. THE MODEL                                            is the percentage of actual segment boundaries hypothesized. Pre-
                                                                          cision is the percentage of hypothesized boundaries that are actual.
The language models used were of the N-gram type commonly used            False Alarms (FA) are the fraction of potential boundaries incor-
in speech recognition [5]. In N-gram models, a word wn from a             rectly hypothesized as boundaries. Segment Error Rate (SER) is the
n 1 word history w1 : : : wn 1. If the history contains a segment         percentage of actual segments identiﬁed without intervening false
boundary <s>, it is truncated before that location. During testing,       alarms.
the model is run as a hidden segment model, hypothesizing segment
boundaries between any two words and implicitly computing the             As can be seen, word context alone can identify a majority of seg-
probabilities of all possible segmentations.                              ment boundaries at a modest false alarm rate of about 2%. The tri-
                                                                          gram model does better than the bigram, but this is expected since it
Associated with each word position are two states, S and NO-S, cor-       has access to a larger context around potential segment boundaries.
responding to a segment starting or not before that word. A forward       to use in its decision. Given these results, we only consider trigram
                                                                          models in all following experiments.
    1Although complete and disﬂuent sentences were marked differently in
the corpus, we modeled these with a single type of boundary token.
5.2. Using Turn Information                                                  boundaries provide some of the strongest cues for these boundaries.
                                                                             Apart from these strong lexical cues, it seems to be helpful to ab-
Next we examined a richer model that incorporated information                stract from word identity and use POS information instead. In other
about the turn-taking between speakers.2 Note that turn boundaries           words, the tag set could be optimized to provide the right level of
are already present in acoustic segmentations, but in this case we           resolution for the segmentation task.
will only use them as a cue to the identiﬁcation of linguistic seg-
ments. Turn information is easily incorporated into the segmenta-            It should be noted that the results for POS-based models are op-
tion model by placing special tags at turn boundaries (in both train-        timistic in the sense that for an actual application one would ﬁrst
ing and testing). Model performance is summarized in Table 2.                have to tag the input with POS labels, and then apply the segmenta-
                                                                             tion model. The actual performance would be degraded by tagging
Table 2: Segmentation performance using turn information                     errors.

Model     Recall Precision FA SER                                            5.4. Error Trade-offs

Baseline  70.2% 60.7% 2.0% 53.1%                                             As an aside to our search for useful features for the segmenta-
                                                                             tion task, we observe that we can optimize any particular language
Turn-tagged 76.9% 66.9% 1.8% 44.9%                                           model by trading off recall performance for false alarm rate, or vice
                                                                             versa. We did this by biasing the likelihoods of S states by some
As can be seen, adding turn information improves performance on              constant factor, causing the Viterbi algorithm to choose these states
all metrics. This improvement occurs even though turn boundaries             more often. Table 4 compares two bias values, and shows that the
are far from perfectly correlated with segment boundaries. As illus-         bias can be used to increase both recall and precision, while also
trated earlier, turns can contain multiple segments, or segments may         reducing the segment error rate.
span multiple turns.
                                                                                       Table 4: Biasing segmentation   SER
5.3. Using Part-of-Speech Information                                        Model Recall Precision FA                44.9%
                                                                             Bias = 1 76.9% 66.9% 1.8%                37.4%
So far we have used only the identity of words. It is likely that            Bias = 2 85.2% 69.2% 2.7%
segmentation is closely related to syntactic (as opposed to lexical)
structure. Short of using a full-scale parser on the input we could                            6. DISCUSSION
use the parts of speech (POS) of words as a more suitable represen-
tation from which to predict segment boundaries. Parts of speech             6.1. Error Analysis
should also generalize much better to contexts containing N-grams
not observed in the training data (assuming the POS of the words             To understand what type of errors the segmenter makes, we hand-
involved is known).                                                          checked a set of 200 false alarms generated by the baseline trigram
                                                                             model. The most frequent type (34%) of false alarm corresponded
We were able to test this hypothesis by using the POS-tagged ver-            to splitting of segments at sentence-internal clause boundaries, e.g.,
sion of the Switchboard corpus. We built two models based on POS             false alarms triggered by a conjunction that would be likely to start
from this data. Model I had all words replaced by their POS labels           a segment. For example, the <s> in the segmentation
during training and test, and also used turn boundary information.
Model II also used POS labels, but retained the word identities of                  i'm not sure how many active volcanos
certain word classes that were deemed to be particularly relevant to                there are now <s> and and what the amount
segmentation. These retained words include ﬁlled pauses, conjunc-                   of material that they do uh put into the
tions, and certain discourse markers such as “okay,” “so,” “well,”                  atmosphere
etc. Results are shown in Table 3.
                                                                             represents a false alarm, presumably triggered by the following co-
Table 3: Segmentation performance using POS information                      ordinating conjunction “and.”

Model     Recall Precision FA SER                                            5% of the false alarms could be attributed to ﬁlled pauses at the
                                                                             end of segments, which were often attached to the following seg-
Word-based 76.9% 66.9% 1.8% 44.9%                                            ment. This actually reﬂects a labeling ambiguity that should not be
                                                                             counted as an error. Another 7% of the false alarm we deemed to
POS-based I 68.9% 58.5% 2.0% 59.3%                                           be labeling errors. Thus, a total of 12% of false alarms could be
                                                                             considered to be actually correct.
POS-based II 79.6% 73.5% 0.9% 39.9%
                                                                             6.2. Other Segmentation Algorithms
We see that POS tags alone (Model I) do not result in better segmen-
tations than words. The fact that Model II performs better than both         Our language-model-based segmentation algorithm is only one of
the all-word based model and the pure POS model indicates that               many that could be used to perform the linguistic segmentation task,
certain function words that tend to occur in the context of segment          given a set of features. Conceptually, segmentation is just another

    2Speakers can talk over each other. We did not model this case sepa-
rately; instead, we adopted the serialization of turns implied by the tran-
scripts.
classiﬁcation problem, in which each word transition must be la-                              8. REFERENCES
beled as either a segment boundary or a within-segment transition.
Two natural choices for alternative approaches are decision trees             1. A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. A max-
and a transformation-based, error-driven classiﬁer of the type de-                imum entropy approach to natural language processing. Com-
veloped by Eric Brill for other tagging problems [2]. Both of these               putational Linguistics, 22(1):39–71, 1996.
methods would make it easier to combine diverse input features that
are not readily integrated into a single probabilistic language model,        2. E. Brill. Some advances in transformation-based part of speech
e.g., if we wanted to use both POS and word identity for each word.3              tagging. In Proceedings of the 12th National Conference on
Our approach, on the other hand, has the advantage of simplicity                  Artiﬁcial Intelligence, Seattle, WA, 1994. AAAI Press.
and efﬁciency. Furthermore, the language model used for segmen-
tation can also be used for speech decoding or rescoring.                     3. K. W. Church. A stochastic parts program and noun phrase
                                                                                  parser for unrestricted text. In Second Conference on Applied
We already mentioned that if POS information is to be used for                    Natural Language Processing, pages 136–143, Austin, Texas,
segmentation, an automatic tagging step is required. This presents                1988.
somewhat of a chicken-and-egg problem, in that taggers typically
rely on segmentations. An appealing solution to this problem in the           4. J. J. Godfrey, E. C. Holliman, and J. McDaniel. SWITCH-
statistical tagging framework [3] would be to model both segmen-                  BOARD: Telephone speech corpus for research and develop-
tation and tag assignment as a single hidden Markov process.                      ment. In Proceedings IEEE Conference on Acoustics, Speech
                                                                                  and Signal Processing, volume I, pages 517–520, San Fran-
6.3. Other Features for Segmentation                                              cisco, March 1992.

All of our experiments were based on lexical information only. To             5. F. Jelinek. Self-organized language modeling for speech recog-
further improve segmentation performance, and to make it less de-                 nition. In A. Waibel and K.-F. Lee, editors, Readings in Speech
pendent on accurate speech recognition, we plan to combine the LM                 Recognition. Morgan Kaufmann, San Mateo, Ca., 1990.
approach with a model for various acoustic and prosodic correlates
of segmentation. These include:                                               6. S. M. Katz. Estimation of probabilities from sparse data for
                                                                                  the language model component of a speech recognizer. IEEE
      Unﬁlled pause durations                                                     Transactions on Acoustics, Speech, and Signal Processing,
                                                                                  35(3):400–401, March 1987.
      Fundamental frequency patterns
                                                                              7. M. Meteer et al. Dysﬂuency annotation stylebook for the
      Phone durations                                                             Switchboard corpus. Distributed by LDC, February 1995. Re-
                                                                                  vised June 1995 by Ann Taylor.
      Glottalization
                                                                              8. M. Meteer and R. Iyer. Modeling conversational speech for
Our current segmentation model deals with each conversation side                  speech recognition. In Proceedings of the Conference on Em-
in isolation. An alternative approach is to model the two sides                   pirical Methods in Natural Language Processing, Philadelphia,
jointly, thereby allowing us to capitalize on correlations between the            PA, May 1996.
segment structure of one speaker and what is said by the other. It is
likely, for example, that backchannel responses would be modeled              9. E. Shriberg and A. Stolcke. Word predictability after hesi-
better this way.                                                                  tations: A corpus-based study. In Proceedings International
                                                                                  Conference on Spoken Language Processing, Philadelphia, PA,
                7. CONCLUSIONS                                                    October 1996.

We have argued for the need for automatic speech segmentation al-            10. A. Stolcke and E. Shriberg. Statistical language modeling
gorithms that can identify linguistically motivated, sentence-level               for speech disﬂuencies. In Proceedings IEEE Conference on
units of speech. We have shown that transcribed speech can be                     Acoustics, Speech and Signal Processing, volume I, pages 405–
segmented linguistically with good accuracy by using an N-gram                    408, Atlanta, GA, May 1996.
language model for the locations of the hidden segment boundaries.
We studied several word-level features for possible incorporation
in the model, and found that best performance so far was achieved
with a combination of function ‘cue’ words, POS labels, and turn
markers.

                  Acknowledgments

This research was supported by DARPA and NSF, under NSF Grant
IRI-9314967. The views herein are those of the authors and should
not be interpreted as representing the policies of DARPA or the
NSF.

    3Such an integration can be achieved in a language model using the max-
imum entropy paradigm [1], but this would make the estimation process
considerably more expensive.
